{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598878273545",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizerFast, TFRobertaModel, TFRobertaForSequenceClassification, RobertaForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"./\", \"Data\")\n",
    "MODELS_PATH = os.path.join(\"./\", \"Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"corefx-issues-train.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         ID                        Area  \\\n0     29338             area-System.Net   \n1     29337             area-System.Net   \n2     29334             area-System.Net   \n3     29331         area-Infrastructure   \n4     29329  area-System.ComponentModel   \n...     ...                         ...   \n1610  26959         area-System.Runtime   \n1611  26957             area-System.Net   \n1612  26956        area-System.Numerics   \n1613  26954         area-System.Runtime   \n1614  26952              area-System.IO   \n\n                                                  Title  \\\n0     Include fragment and query in Uri.LocalPath on...   \n1     Unify setting null CookieContainer behavior on...   \n2     Check URI scheme length only after verifying t...   \n3     [Perf] Ubuntu16.04 runs blocked by multiple \"P...   \n4     Port System.ComponentModel.Composition.Registr...   \n...                                                 ...   \n1610  Re-evaluate default buffer size for getpw nati...   \n1611  Validate ClientWebSocket wss connections work ...   \n1612           Add Quaternion.Divide(Quaternion, float)   \n1613  Proposal: TryForSufficientStack method to supp...   \n1614  System.IO.IOException: 'Entries cannot be open...   \n\n                                            Description  \n0     While testing XmlUriResolver, @pjanotti discov...  \n1     For HttpClientHandler layer (above the WinHttp...  \n2     URI construction is failing on valid URIs unde...  \n3     [perf_ubuntu16.04_release](https://ci2.dot.net...  \n4     Greetings,    regarding [Port System.Component...  \n...                                                 ...  \n1610  By default, we are allocating 1K of memory on ...  \n1611  After https://github.com/dotnet/corefx/pull/26...  \n1612  I noticed that Quaternion.Divide Method has no...  \n1613  _From @kkokosa on February 8, 2018 12:8_  Due ...  \n1614  I am moving an application from .net to core. ...  \n\n[1615 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Area</th>\n      <th>Title</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>29338</td>\n      <td>area-System.Net</td>\n      <td>Include fragment and query in Uri.LocalPath on...</td>\n      <td>While testing XmlUriResolver, @pjanotti discov...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>29337</td>\n      <td>area-System.Net</td>\n      <td>Unify setting null CookieContainer behavior on...</td>\n      <td>For HttpClientHandler layer (above the WinHttp...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>29334</td>\n      <td>area-System.Net</td>\n      <td>Check URI scheme length only after verifying t...</td>\n      <td>URI construction is failing on valid URIs unde...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29331</td>\n      <td>area-Infrastructure</td>\n      <td>[Perf] Ubuntu16.04 runs blocked by multiple \"P...</td>\n      <td>[perf_ubuntu16.04_release](https://ci2.dot.net...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>29329</td>\n      <td>area-System.ComponentModel</td>\n      <td>Port System.ComponentModel.Composition.Registr...</td>\n      <td>Greetings,    regarding [Port System.Component...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1610</th>\n      <td>26959</td>\n      <td>area-System.Runtime</td>\n      <td>Re-evaluate default buffer size for getpw nati...</td>\n      <td>By default, we are allocating 1K of memory on ...</td>\n    </tr>\n    <tr>\n      <th>1611</th>\n      <td>26957</td>\n      <td>area-System.Net</td>\n      <td>Validate ClientWebSocket wss connections work ...</td>\n      <td>After https://github.com/dotnet/corefx/pull/26...</td>\n    </tr>\n    <tr>\n      <th>1612</th>\n      <td>26956</td>\n      <td>area-System.Numerics</td>\n      <td>Add Quaternion.Divide(Quaternion, float)</td>\n      <td>I noticed that Quaternion.Divide Method has no...</td>\n    </tr>\n    <tr>\n      <th>1613</th>\n      <td>26954</td>\n      <td>area-System.Runtime</td>\n      <td>Proposal: TryForSufficientStack method to supp...</td>\n      <td>_From @kkokosa on February 8, 2018 12:8_  Due ...</td>\n    </tr>\n    <tr>\n      <th>1614</th>\n      <td>26952</td>\n      <td>area-System.IO</td>\n      <td>System.IO.IOException: 'Entries cannot be open...</td>\n      <td>I am moving an application from .net to core. ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1615 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [ID, Area, Title, Description]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Area</th>\n      <th>Title</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['area-System.Net', 'area-Infrastructure',\n       'area-System.ComponentModel', 'area-System.Security',\n       'area-System.Runtime', 'area-System.IO', 'area-System.Xml',\n       'area-System.Collections', 'area-System.Threading',\n       'area-System.Reflection', 'area-System.Memory',\n       'area-System.Diagnostics', 'area-Serialization',\n       'area-System.Drawing', 'area-Meta', 'area-System.Data',\n       'area-Microsoft.CSharp', 'area-System.Numerics',\n       'area-System.Text', 'area-System.Globalization',\n       'area-System.Linq', 'area-System.Console'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.Area.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = {}\n",
    "for i, area in enumerate(df.Area.unique()):\n",
    "    lookup[area] = i\n",
    "    lookup[i] = area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'area-System.Net': 0,\n 0: 'area-System.Net',\n 'area-Infrastructure': 1,\n 1: 'area-Infrastructure',\n 'area-System.ComponentModel': 2,\n 2: 'area-System.ComponentModel',\n 'area-System.Security': 3,\n 3: 'area-System.Security',\n 'area-System.Runtime': 4,\n 4: 'area-System.Runtime',\n 'area-System.IO': 5,\n 5: 'area-System.IO',\n 'area-System.Xml': 6,\n 6: 'area-System.Xml',\n 'area-System.Collections': 7,\n 7: 'area-System.Collections',\n 'area-System.Threading': 8,\n 8: 'area-System.Threading',\n 'area-System.Reflection': 9,\n 9: 'area-System.Reflection',\n 'area-System.Memory': 10,\n 10: 'area-System.Memory',\n 'area-System.Diagnostics': 11,\n 11: 'area-System.Diagnostics',\n 'area-Serialization': 12,\n 12: 'area-Serialization',\n 'area-System.Drawing': 13,\n 13: 'area-System.Drawing',\n 'area-Meta': 14,\n 14: 'area-Meta',\n 'area-System.Data': 15,\n 15: 'area-System.Data',\n 'area-Microsoft.CSharp': 16,\n 16: 'area-Microsoft.CSharp',\n 'area-System.Numerics': 17,\n 17: 'area-System.Numerics',\n 'area-System.Text': 18,\n 18: 'area-System.Text',\n 'area-System.Globalization': 19,\n 19: 'area-System.Globalization',\n 'area-System.Linq': 20,\n 20: 'area-System.Linq',\n 'area-System.Console': 21,\n 21: 'area-System.Console'}"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Area = df.Area.apply(lambda x: lookup[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0        0\n1        0\n2        0\n3        1\n4        2\n        ..\n1610     4\n1611     0\n1612    17\n1613     4\n1614     5\nName: Area, Length: 1615, dtype: int64"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df.Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.Area\n",
    "titles = df.Title\n",
    "descriptions = df.Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for title, description in zip(titles, descriptions):\n",
    "    text.append(\" \".join(title.split()) + \" \" + \" \".join(description.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1615"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1615"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "236"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "len(max(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[\"Include fragment and query in Uri.LocalPath on Unix While testing XmlUriResolver, @pjanotti discovered that any segments of a file path following a '#' symbol will be cut out of Uri.LocalPath on Unix. Based on additional tests, this also occurs for the '?' symbol. This is happening because the Unix specific case for local path only uses the path component of the URI: https://github.com/dotnet/corefx/blob/9e8d443ff78c4f0a9a6bedf7f95961cf96ceff0a/src/System.Private.Uri/src/System/Uri.cs#L1032-L1037 The fix here is to include the fragment and query in LocalPath in the Unix path specific case. This PR enables the test case in XmlUriResolver that uncovered this issues, and adds some additional cases to our URI tests. Fixes: #28486\",\n \"Unify setting null CookieContainer behavior on HttpClientHandler For HttpClientHandler layer (above the WinHttpHandler layer on Windows), we should be consistent and throw the exception in the CookieContainer setter when null value is provided, to match .NET Framework's behavior. This would keep the same behavior across all platforms for the setter of the HttpClientHandler.CookieContainer property. Close: #3205\",\n 'Check URI scheme length only after verifying the scheme contains valid characters URI construction is failing on valid URIs under the following conditions: - An absolute URI is constructed using the `Uri(Uri absolute, string relative)` constructor. - The relative string begins with 1024+ characters, followed by a colon. - The relative string contains but does not begin with a forward or back slash. See the test added in this PR for an example URI. The fix is to check the scheme length _after_ validating that the potential scheme contains only legal characters (ie, not a forward or back slash). This keeps us from running into the situation above, where the relative URI contains a colon that is unambiguously not a scheme separator because the \"scheme\" is actually just a path that contains a colon. This fix improves the correctness of our relative path parsing at the cost of an additional stacalloc in the case where the relative part of the URI is really an absolute URI with a scheme length >= 1024. Fixes: #29011 Details below: --------- When we construct an absolute URI from a relative URI, the first thing we try to do is parse the relative URI as an absolute URI. That parsing process returns an error code that we use to determine what happens next. The parsing errors are as follows: https://github.com/dotnet/corefx/blob/bffef76f6af208e2042a2f27bc081ee908bb390b/src/System.Private.Uri/src/System/UriEnumTypes.cs#L67-L93 If we successfully parse an absolute URI (error = None), we return that URI and ignore the absolute URI we were passed. That might seem a little odd, but it\\'s a useful behavior in practice. If we get an error that is less than `LastRelativeUriOkErrIndex`(see the code above), we attempt to create a relative URI from the string and then root it with the absolute URI provided. If we return any other error, we believe that the string is neither a valid relative or absolute URI and throw an exception. In this case, PrivateParseMinimal is returning the error `SchemeLimit`, which indicates that we have too large of a scheme. As documented in the code above, this isn\\'t considered a recoverable error. The relative string provided has some characters that are clearly invalid in a scheme, so the real error we should be returning from TryParse is `InvalidScheme`. Since `InvalidScheme` is less than `LastRelativeUriOkErrIndex`, we will then be able to create a relative URI. Fixing the returned parsing error allows this URI to be constructed successfully.',\n '[Perf] Ubuntu16.04 runs blocked by multiple \"PE file is already strong-name signed.\" errors [perf_ubuntu16.04_release](https://ci2.dot.net/job/dotnet_corefx/job/perf/job/master/job/perf_ubuntu16.04_release/1715/consoleText) ```bash bash ./build-managed.sh -release -tests -- /p:Performance=true /p:TargetOS=Linux /m:1 /p:LogToBenchview=true /p:BenchviewRunType=rolling /p:PerformanceType=Profile ``` ... /home/administrator/jenkins/w/dotnet_corefx/perf/master/perf_ubuntu16.04_release/Tools/sign.targets(113,5): error : /home/administrator/jenkins/w/dotnet_corefx/perf/master/perf_ubuntu16.04_release/bin/obj/ref/System.Threading.Tasks.Dataflow/4.6.3.0/netstandard/System.Threading.Tasks.Dataflow.dll: PE file is already strong-name signed. [/home/administrator/jenkins/w/dotnet_corefx/perf/master/perf_ubuntu16.04_release/src/System.Threading.Tasks.Dataflow/ref/System.Threading.Tasks.Dataflow.csproj] 0 Warning(s) 32 Error(s)',\n 'Port System.ComponentModel.Composition.Registration (MEF1) to .NET Core Greetings, regarding [Port System.ComponentModel.Composition (MEF1) to .NET Core](https://github.com/dotnet/corefx/issues/11857) the following is still missing: - `System.ComponentModel.Composition.Registration.RegistrationBuilder` - `System.ComponentModel.Composition.Registration.PartBuilder` - `System.ComponentModel.Composition.Registration.PartBuilder<>` See also https://github.com/dotnet/corefx/issues/11857#issuecomment-382342975 edit by @ViktorHofer: fixed link.']"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    0\n1    0\n2    0\n3    1\n4    2\nName: Area, dtype: int64"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['area-System.Net',\n 'area-System.Net',\n 'area-System.Net',\n 'area-Infrastructure',\n 'area-System.ComponentModel']"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "[lookup[x] for x in labels[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "df = pd.DataFrame(list(zip(text, labels)), columns =['Text', 'Label'])\n",
    "df.to_csv(os.path.join(DATA_PATH, \"cleaned_train.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_dataset = tf.data.experimental.CsvDataset(os.path.join(DATA_PATH, \"cleaned_train.csv\"), [tf.string, tf.int32], header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_dataset = github_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_py_function(sentences):\n",
    "    decoded = []\n",
    "    for sentence in sentences.numpy():\n",
    "        decoded.append(sentence.decode())\n",
    "    encoded = tokenizer(decoded, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='tf')\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "    attention_mask = encoded[\"attention_mask\"]\n",
    "    return (input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(batch_x, batch_y):\n",
    "    input_ids, attention_mask = tf.py_function(tf_py_function, [batch_x], (tf.int32, tf.int32))\n",
    "    input_ids.set_shape([None, MAX_LEN])\n",
    "    attention_mask.set_shape([None, MAX_LEN])\n",
    "    return (input_ids, attention_mask, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_dict(input_ids, attention_mask, labels):\n",
    "    return ({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dataset = github_dataset.map(encode_data, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(map_to_dict, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_train = new_dataset.take(int(len(labels) // BATCH_SIZE *0.8)).cache()\n",
    "github_valid = new_dataset.skip(int(len(labels) // BATCH_SIZE *0.8)).cache()\n",
    "github_train = github_train.repeat().prefetch(1)\n",
    "github_valid = github_valid.repeat().prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "github_model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "\n",
    "github_model.compile(optimizer=opt, loss=loss, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\nWARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n40/40 [==============================] - 19s 471ms/step - loss: 2.7686 - accuracy: 0.2609 - val_loss: 2.4024 - val_accuracy: 0.3375\nEpoch 2/10\n40/40 [==============================] - 17s 429ms/step - loss: 2.4928 - accuracy: 0.2898 - val_loss: 2.1860 - val_accuracy: 0.3812\nEpoch 3/10\n40/40 [==============================] - 17s 433ms/step - loss: 2.1084 - accuracy: 0.4477 - val_loss: 1.8023 - val_accuracy: 0.5094\nEpoch 4/10\n40/40 [==============================] - 17s 432ms/step - loss: 1.7602 - accuracy: 0.5383 - val_loss: 1.5091 - val_accuracy: 0.5875\nEpoch 5/10\n40/40 [==============================] - 17s 434ms/step - loss: 1.4499 - accuracy: 0.6500 - val_loss: 1.2797 - val_accuracy: 0.6812\nEpoch 6/10\n40/40 [==============================] - 17s 434ms/step - loss: 1.2206 - accuracy: 0.7352 - val_loss: 1.1179 - val_accuracy: 0.7469\nEpoch 7/10\n40/40 [==============================] - 18s 438ms/step - loss: 1.0348 - accuracy: 0.7828 - val_loss: 0.9940 - val_accuracy: 0.7844\nEpoch 8/10\n40/40 [==============================] - 18s 441ms/step - loss: 0.8641 - accuracy: 0.8336 - val_loss: 0.9014 - val_accuracy: 0.8062\nEpoch 9/10\n40/40 [==============================] - 18s 438ms/step - loss: 0.7375 - accuracy: 0.8609 - val_loss: 0.8245 - val_accuracy: 0.8250\nEpoch 10/10\n40/40 [==============================] - 18s 444ms/step - loss: 0.6351 - accuracy: 0.8945 - val_loss: 0.7839 - val_accuracy: 0.8188\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1baa062f4c8>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "github_model.fit(github_train, validation_data=github_valid, steps_per_epoch= int(int(len(labels) // BATCH_SIZE *0.8)) , epochs=10, validation_steps=int(int(len(labels) // BATCH_SIZE *0.2)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save TF model (Huggingface format and SavedModel format) and create Pytorch model from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('./Models\\\\tf\\\\vocab.json',\n './Models\\\\tf\\\\merges.txt',\n './Models\\\\tf\\\\special_tokens_map.json',\n './Models\\\\tf\\\\added_tokens.json')"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "github_model.save_pretrained(os.path.join(MODELS_PATH, \"tf\"))\n",
    "tokenizer.save_pretrained(os.path.join(MODELS_PATH, \"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n\nAll the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\nIf your task is similar to the task the model of the ckeckpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
    }
   ],
   "source": [
    "pytorch_model = RobertaForSequenceClassification.from_pretrained(os.path.join(MODELS_PATH, \"tf\"), from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('./Models\\\\pt\\\\vocab.json',\n './Models\\\\pt\\\\merges.txt',\n './Models\\\\pt\\\\special_tokens_map.json',\n './Models\\\\pt\\\\added_tokens.json')"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "pytorch_model.save_pretrained(os.path.join(MODELS_PATH, \"pt\"))\n",
    "tokenizer.save_pretrained(os.path.join(MODELS_PATH, \"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "callable = tf.function(github_model.call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_function = callable.get_concrete_function([tf.TensorSpec([None, MAX_LEN], tf.int32, name=\"input_ids\"), tf.TensorSpec([None, MAX_LEN], tf.int32, name=\"attention_mask\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nWARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nINFO:tensorflow:Assets written to: ./Models\\Serving\\1\\assets\n"
    }
   ],
   "source": [
    "tf.saved_model.save(github_model, os.path.join(MODELS_PATH, \"Serving\", \"1\"), signatures=concrete_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try models on test issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = \"AppDomain.SetPrincipalPolicy(PrincipalPolicy.WindowsPrincipal) works only once. Setting the PrincipalPolicy on the current AppDomain to WindowsPrincipal works only for the first thread being started. Any subsequent thread has Thread.CurrentPrincipal evaluated to NULL.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tf = tokenizer(issue, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='tf', add_special_tokens=True, return_token_type_ids=False)\n",
    "encoded_pt = tokenizer(issue, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt', add_special_tokens=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "All model checkpoint weights were used when initializing TFRobertaForSequenceClassification.\n\nAll the weights of TFRobertaForSequenceClassification were initialized from the model checkpoint at ./Models\\tf.\nIf your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
    }
   ],
   "source": [
    "github_model = TFRobertaForSequenceClassification.from_pretrained(os.path.join(MODELS_PATH, \"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(<tf.Tensor: shape=(1, 22), dtype=float32, numpy=\narray([[-0.5988496 ,  1.1280658 ,  0.47194374, -0.50786096,  1.2456827 ,\n         2.5384133 , -0.6554079 , -0.72741085,  0.67490077, -0.09534095,\n        -1.4605486 ,  1.3067842 , -0.6346639 , -0.4347392 ,  0.521743  ,\n         1.1091499 , -0.45329064, -0.6518139 , -0.71086705, -0.8429568 ,\n        -0.49102157, -0.33502215]], dtype=float32)>,)\n"
    }
   ],
   "source": [
    "tf_result = github_model(encoded_tf)\n",
    "print(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(tensor([[-0.5989,  1.1281,  0.4719, -0.5079,  1.2457,  2.5384, -0.6554, -0.7274,\n          0.6749, -0.0953, -1.4605,  1.3068, -0.6347, -0.4347,  0.5217,  1.1091,\n         -0.4533, -0.6518, -0.7109, -0.8430, -0.4910, -0.3350]],\n       grad_fn=<AddmmBackward>),)\n"
    }
   ],
   "source": [
    "pt_result = pytorch_model(input_ids=encoded_pt[\"input_ids\"], attention_mask=encoded_pt[\"attention_mask\"])\n",
    "print(pt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to ONNX and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "pytorch_model.eval()\n",
    "torch.onnx.export(pytorch_model,               # model being run\n",
    "                  (encoded_pt[\"input_ids\"], encoded_pt[\"attention_mask\"]),  # model input (or a tuple for multiple inputs)\n",
    "                  os.path.join(MODELS_PATH, \"roberta_github_issues.onnx\"),   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=12,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input_ids', 'attention_mask'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input_ids' : {0 : 'batch_size'},\n",
    "                                'attention_mask' : {0 : 'batch_size'},\n",
    "                                'output' : {0 : 'batch_size'}}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(os.path.join(MODELS_PATH, \"roberta_github_issues.onnx\"))\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(os.path.join(MODELS_PATH, \"roberta_github_issues.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'attention_mask'"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "ort_session.get_inputs()[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(encoded_pt[\"input_ids\"]), ort_session.get_inputs()[1].name: to_numpy(encoded_pt[\"attention_mask\"])}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[array([[-0.59884876,  1.1280674 ,  0.4719437 , -0.5078648 ,  1.2456806 ,\n         2.538411  , -0.6554067 , -0.7274111 ,  0.67490226, -0.09534154,\n        -1.4605478 ,  1.306784  , -0.6346635 , -0.43473876,  0.5217458 ,\n         1.1091526 , -0.45329082, -0.6518149 , -0.7108661 , -0.8429569 ,\n        -0.49102157, -0.33502263]], dtype=float32)]\n(tensor([[-0.5989,  1.1281,  0.4719, -0.5079,  1.2457,  2.5384, -0.6554, -0.7274,\n          0.6749, -0.0953, -1.4605,  1.3068, -0.6347, -0.4347,  0.5217,  1.1091,\n         -0.4533, -0.6518, -0.7109, -0.8430, -0.4910, -0.3350]],\n       grad_fn=<AddmmBackward>),)\n(<tf.Tensor: shape=(1, 22), dtype=float32, numpy=\narray([[-0.5988496 ,  1.1280658 ,  0.47194374, -0.50786096,  1.2456827 ,\n         2.5384133 , -0.6554079 , -0.72741085,  0.67490077, -0.09534095,\n        -1.4605486 ,  1.3067842 , -0.6346639 , -0.4347392 ,  0.521743  ,\n         1.1091499 , -0.45329064, -0.6518139 , -0.71086705, -0.8429568 ,\n        -0.49102157, -0.33502215]], dtype=float32)>,)\n"
    }
   ],
   "source": [
    "print(ort_outs)\n",
    "print(pt_result)\n",
    "print(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "index : 5, category : area-System.IO\n"
    }
   ],
   "source": [
    "index = np.argmax(to_numpy(pt_result[0]))\n",
    "print(f\"index : {index}, category : {lookup[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.functional.softmax(pt_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.0141, 0.0791, 0.0410, 0.0154, 0.0889, 0.3239, 0.0133, 0.0124, 0.0503,\n         0.0233, 0.0059, 0.0945, 0.0136, 0.0166, 0.0431, 0.0776, 0.0163, 0.0133,\n         0.0126, 0.0110, 0.0157, 0.0183]], grad_fn=<SoftmaxBackward>)"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}