{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601644042309",
   "display_name": "Python 3.7.9 64-bit ('torch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports and global variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "import json\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(os.getcwd(), \"Data\")\n",
    "MODELS_FOLDER = os.path.join(os.getcwd(), \"Models\")\n",
    "TRAIN_FILE_NAME = \"corefx_cleaned_train.csv\"\n",
    "VAL_FILE_NAME = \"corefx_cleaned_val.csv\"\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_FOLDER, TRAIN_FILE_NAME)\n",
    "VAL_FILE_PATH = os.path.join(DATA_FOLDER, VAL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the length of the train set to use in the scheduler\n",
    "train_df = pd.read_csv(os.path.join(DATA_FOLDER, \"corefx_cleaned_train.csv\"))\n",
    "TRAIN_LEN = len(train_df)\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 128\n",
    "DROPOUT = 0.3\n",
    "SEED = 2020\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "source": [
    "# Define Dataset class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorefxDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, csv_path: str, tokenizer, max_len: int = 128):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = self.tokenizer(self.df[\"Text\"].iloc[idx], padding='max_length', truncation=True, max_length=self.max_len)\n",
    "        label = self.df[\"Label\"].iloc[idx]\n",
    "        return ({\n",
    "            'input_ids' : torch.tensor(encoded['input_ids'], dtype=torch.long),\n",
    "            'attention_mask' : torch.tensor(encoded['attention_mask'], dtype=torch.long)},\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "            )\n"
   ]
  },
  {
   "source": [
    "# Define DataModule class (specific to pytorch lightning)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorefxIssuesDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, data_path: str, tokenizer, max_len: int = 128, batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_split = CorefxDataset(os.path.join(self.data_path, \"corefx_cleaned_train.csv\"), self.tokenizer, self.max_len)\n",
    "        return torch.utils.data.DataLoader(train_split, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_split = CorefxDataset(os.path.join(self.data_path, \"corefx_cleaned_val.csv\"), self.tokenizer, self.max_len)\n",
    "        return torch.utils.data.DataLoader(val_split, batch_size=self.batch_size, shuffle=True, num_workers=0, )"
   ]
  },
  {
   "source": [
    "# Define the model class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorefxRobertaModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_labels, dropout):\n",
    "        super().__init__()\n",
    "        self.lr = 3e-5\n",
    "        self.hparams.num_labels = num_labels\n",
    "        self.hparams.dropout = dropout\n",
    "        self.save_hyperparameters()\n",
    "        self.roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_labels, return_dict=True, hidden_dropout_prob=dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.roberta_model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # All this is taken from Huggingface in how they use this for BERT\n",
    "        # Since Roberta is basically BERT, I keep the same\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        num_train_steps = int(TRAIN_LEN / BATCH_SIZE * EPOCHS)\n",
    "        optimizer = AdamW(optimizer_parameters, lr=self.lr)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.roberta_model.train()\n",
    "        x, y = batch\n",
    "        outputs = self(x[\"input_ids\"], x[\"attention_mask\"])\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs.logits, y)\n",
    "        acc = FM.accuracy(outputs.logits, y, num_classes=self.hparams.num_labels)\n",
    "        result = pl.TrainResult(loss)\n",
    "        result.log_dict({'train_acc': acc, 'train_loss': loss}, prog_bar=True)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.roberta_model.eval()\n",
    "        x, y = batch\n",
    "        outputs = self(x[\"input_ids\"], x[\"attention_mask\"])\n",
    "        val_loss = torch.nn.CrossEntropyLoss()(outputs.logits, y)\n",
    "        acc = FM.accuracy(outputs.logits, y, num_classes=self.hparams.num_labels)\n",
    "        result = pl.EvalResult(checkpoint_on=val_loss, early_stop_on=val_loss)\n",
    "        result.log_dict({'val_acc': acc, 'val_loss': val_loss}, prog_bar=True)\n",
    "        return result"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corefx_dm = CorefxIssuesDataModule(DATA_FOLDER, roberta_tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, 'lookup.json')) as json_file: \n",
    "    lookup = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;, &#39;roberta.pooler.dense.weight&#39;, &#39;roberta.pooler.dense.bias&#39;]\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;]\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "roberta = CorefxRobertaModel(num_labels=len(lookup.keys()) // 2, dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   min_delta=0.00,\n",
    "   patience=20,\n",
    "   verbose=True,\n",
    "   mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.path.join(MODELS_FOLDER, \"pl\", \"best.ckpt\"),\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "GPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name          | Type                             | Params\n-------------------------------------------------------------------\n0 | roberta_model | RobertaForSequenceClassification | 124 M \nEpoch 0:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.97it/s, loss=2.491, v_num=0]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 0:  81%|████████  | 42/52 [00:13&lt;00:03,  3.01it/s, loss=2.491, v_num=0]\nEpoch 0:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.06it/s, loss=2.491, v_num=0]\nEpoch 0:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.10it/s, loss=2.491, v_num=0]\nEpoch 0:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.14it/s, loss=2.491, v_num=0]\nEpoch 0:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.18it/s, loss=2.491, v_num=0]\nEpoch 0:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.22it/s, loss=2.491, v_num=0]\nEpoch 0:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.26it/s, loss=2.491, v_num=0]\nEpoch 0:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.30it/s, loss=2.491, v_num=0]\nEpoch 0:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.34it/s, loss=2.491, v_num=0]\nEpoch 0:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.38it/s, loss=2.491, v_num=0]\nEpoch 00000: val_checkpoint_on reached 2.58870 (best 2.58870), saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_0.ckpt as top 1\nEpoch 0: 100%|██████████| 52/52 [00:25&lt;00:00,  2.01it/s, loss=2.491, v_num=0, val_acc=0.27, val_loss=2.59]\nEpoch 1:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.98it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 1:  81%|████████  | 42/52 [00:13&lt;00:03,  3.03it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nEpoch 1:  83%|████████▎ | 43/52 [00:13&lt;00:02,  3.07it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nEpoch 1:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.12it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nEpoch 1:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.16it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nEpoch 1:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.20it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nEpoch 1:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.24it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nEpoch 1:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.28it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nEpoch 1:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.32it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nEpoch 1:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.36it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nEpoch 1:  98%|█████████▊| 51/52 [00:14&lt;00:00,  3.40it/s, loss=2.364, v_num=0, val_acc=0.27, val_loss=2.59, train_acc=0.264, train_loss=2.68]\nEpoch 00001: val_checkpoint_on reached 2.13489 (best 2.13489), saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_1.ckpt as top 1\nEpoch 1: 100%|██████████| 52/52 [00:26&lt;00:00,  1.93it/s, loss=2.364, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.264, train_loss=2.68]\nEpoch 2:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.99it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 2:  81%|████████  | 42/52 [00:13&lt;00:03,  3.03it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nEpoch 2:  83%|████████▎ | 43/52 [00:13&lt;00:02,  3.08it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nEpoch 2:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.12it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nEpoch 2:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.16it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nEpoch 2:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.21it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nEpoch 2:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.25it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nEpoch 2:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.29it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nEpoch 2:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.33it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nEpoch 2:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.37it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nEpoch 2:  98%|█████████▊| 51/52 [00:14&lt;00:00,  3.41it/s, loss=2.002, v_num=0, val_acc=0.355, val_loss=2.13, train_acc=0.311, train_loss=2.44]\nEpoch 00002: val_checkpoint_on reached 1.86081 (best 1.86081), saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_2.ckpt as top 1\nEpoch 2: 100%|██████████| 52/52 [00:25&lt;00:00,  2.03it/s, loss=2.002, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.311, train_loss=2.44]\nEpoch 3:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.96it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 3:  81%|████████  | 42/52 [00:13&lt;00:03,  3.01it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nEpoch 3:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.05it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nEpoch 3:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.09it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nEpoch 3:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.14it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nEpoch 3:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.18it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nEpoch 3:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.22it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nEpoch 3:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.26it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nEpoch 3:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.30it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nEpoch 3:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.34it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nEpoch 3:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.38it/s, loss=1.785, v_num=0, val_acc=0.506, val_loss=1.86, train_acc=0.405, train_loss=2.1]\nEpoch 00003: val_checkpoint_on reached 1.57025 (best 1.57025), saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_3.ckpt as top 1\nEpoch 3: 100%|██████████| 52/52 [00:27&lt;00:00,  1.89it/s, loss=1.785, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.405, train_loss=2.1] \nEpoch 4:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.98it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 4:  81%|████████  | 42/52 [00:13&lt;00:03,  3.02it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nEpoch 4:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.06it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nEpoch 4:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.11it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nEpoch 4:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.15it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nEpoch 4:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.19it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nEpoch 4:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.24it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nEpoch 4:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.28it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nEpoch 4:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.32it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nEpoch 4:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.35it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nEpoch 4:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.39it/s, loss=1.512, v_num=0, val_acc=0.58, val_loss=1.57, train_acc=0.492, train_loss=1.85]\nEpoch 00004: val_checkpoint_on reached 1.48362 (best 1.48362), saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_4.ckpt as top 1\nEpoch 4: 100%|██████████| 52/52 [00:25&lt;00:00,  2.04it/s, loss=1.512, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.492, train_loss=1.85]\nEpoch 5:  79%|███████▉  | 41/52 [00:13&lt;00:03,  3.01it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 5:  81%|████████  | 42/52 [00:13&lt;00:03,  3.05it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nEpoch 5:  83%|████████▎ | 43/52 [00:13&lt;00:02,  3.10it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nEpoch 5:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.14it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nEpoch 5:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.18it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nEpoch 5:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.23it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nEpoch 5:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.27it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nEpoch 5:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.31it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nEpoch 5:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.35it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nEpoch 5:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.39it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nEpoch 5:  98%|█████████▊| 51/52 [00:14&lt;00:00,  3.43it/s, loss=1.368, v_num=0, val_acc=0.62, val_loss=1.48, train_acc=0.597, train_loss=1.57]\nEpoch 00005: val_checkpoint_on reached 1.37046 (best 1.37046), saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_5.ckpt as top 1\nEpoch 5: 100%|██████████| 52/52 [00:26&lt;00:00,  1.94it/s, loss=1.368, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.597, train_loss=1.57]\nEpoch 6:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.98it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 6:  81%|████████  | 42/52 [00:13&lt;00:03,  3.03it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nEpoch 6:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.07it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nEpoch 6:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.12it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nEpoch 6:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.16it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nEpoch 6:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.20it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nEpoch 6:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.24it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nEpoch 6:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.28it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nEpoch 6:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.32it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nEpoch 6:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.36it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nEpoch 6:  98%|█████████▊| 51/52 [00:14&lt;00:00,  3.40it/s, loss=1.183, v_num=0, val_acc=0.655, val_loss=1.37, train_acc=0.631, train_loss=1.4]\nEpoch 00006: val_checkpoint_on  was not in top 1\nEpoch 6: 100%|██████████| 52/52 [00:15&lt;00:00,  3.46it/s, loss=1.183, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.631, train_loss=1.4]\nEpoch 7:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.98it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 7:  81%|████████  | 42/52 [00:13&lt;00:03,  3.03it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nEpoch 7:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.07it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nEpoch 7:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.11it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nEpoch 7:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.16it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nEpoch 7:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.20it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nEpoch 7:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.24it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nEpoch 7:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.28it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nEpoch 7:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.32it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nEpoch 7:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.36it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nEpoch 7:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.40it/s, loss=1.050, v_num=0, val_acc=0.644, val_loss=1.44, train_acc=0.689, train_loss=1.21]\nEpoch 00007: val_checkpoint_on reached 1.05070 (best 1.05070), saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_7.ckpt as top 1\nEpoch 7: 100%|██████████| 52/52 [00:25&lt;00:00,  2.04it/s, loss=1.050, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.689, train_loss=1.21]\nEpoch 8:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.95it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 8:  81%|████████  | 42/52 [00:14&lt;00:03,  2.99it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nEpoch 8:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.04it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nEpoch 8:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.08it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nEpoch 8:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.12it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nEpoch 8:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.16it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nEpoch 8:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.21it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nEpoch 8:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.25it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nEpoch 8:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.28it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nEpoch 8:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.32it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nEpoch 8:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.36it/s, loss=1.012, v_num=0, val_acc=0.722, val_loss=1.05, train_acc=0.723, train_loss=1.08]\nEpoch 00008: val_checkpoint_on  was not in top 1\nEpoch 8: 100%|██████████| 52/52 [00:15&lt;00:00,  3.42it/s, loss=1.012, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.723, train_loss=1.08]\nEpoch 9:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.97it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 9:  81%|████████  | 42/52 [00:13&lt;00:03,  3.01it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nEpoch 9:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.06it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nEpoch 9:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.10it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nEpoch 9:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.14it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nEpoch 9:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.18it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nEpoch 9:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.22it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nEpoch 9:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.26it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nEpoch 9:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.30it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nEpoch 9:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.34it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nEpoch 9:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.38it/s, loss=0.853, v_num=0, val_acc=0.681, val_loss=1.22, train_acc=0.743, train_loss=1.01]\nEpoch 00009: val_checkpoint_on reached 0.99613 (best 0.99613), saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_9.ckpt as top 1\nEpoch 9: 100%|██████████| 52/52 [00:27&lt;00:00,  1.89it/s, loss=0.853, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.743, train_loss=1.01]\nEpoch 10:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.98it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 10:  81%|████████  | 42/52 [00:13&lt;00:03,  3.02it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nEpoch 10:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.07it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nEpoch 10:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.11it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nEpoch 10:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.15it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nEpoch 10:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.19it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nEpoch 10:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.23it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nEpoch 10:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.28it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nEpoch 10:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.32it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nEpoch 10:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.35it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nEpoch 10:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.39it/s, loss=0.784, v_num=0, val_acc=0.739, val_loss=0.996, train_acc=0.783, train_loss=0.859]\nEpoch 00010: val_checkpoint_on  was not in top 1\nEpoch 10: 100%|██████████| 52/52 [00:15&lt;00:00,  3.45it/s, loss=0.784, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.783, train_loss=0.859]  \nEpoch 11:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.95it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 11:  81%|████████  | 42/52 [00:14&lt;00:03,  2.99it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nEpoch 11:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.03it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nEpoch 11:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.08it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nEpoch 11:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.12it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nEpoch 11:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.16it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nEpoch 11:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.20it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nEpoch 11:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.24it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nEpoch 11:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.28it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nEpoch 11:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.32it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nEpoch 11:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.36it/s, loss=0.645, v_num=0, val_acc=0.723, val_loss=1.1, train_acc=0.808, train_loss=0.773]\nEpoch 00011: val_checkpoint_on  was not in top 1\nEpoch 11: 100%|██████████| 52/52 [00:15&lt;00:00,  3.42it/s, loss=0.645, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.808, train_loss=0.773]\nEpoch 12:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.94it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 12:  81%|████████  | 42/52 [00:14&lt;00:03,  2.98it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nEpoch 12:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.02it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nEpoch 12:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.06it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nEpoch 12:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.10it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nEpoch 12:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.14it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nEpoch 12:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.18it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nEpoch 12:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.22it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nEpoch 12:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.26it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nEpoch 12:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.30it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nEpoch 12:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.34it/s, loss=0.676, v_num=0, val_acc=0.734, val_loss=1.03, train_acc=0.831, train_loss=0.676]\nEpoch 00012: val_checkpoint_on reached 0.93089 (best 0.93089), saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_12.ckpt as top 1\nEpoch 12: 100%|██████████| 52/52 [00:25&lt;00:00,  2.01it/s, loss=0.676, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.831, train_loss=0.676]\nEpoch 13:  79%|███████▉  | 41/52 [00:13&lt;00:03,  2.97it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 13:  81%|████████  | 42/52 [00:13&lt;00:03,  3.01it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nEpoch 13:  83%|████████▎ | 43/52 [00:14&lt;00:02,  3.06it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nEpoch 13:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.10it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nEpoch 13:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.14it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nEpoch 13:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.19it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nEpoch 13:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.23it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nEpoch 13:  92%|█████████▏| 48/52 [00:14&lt;00:01,  3.27it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nEpoch 13:  94%|█████████▍| 49/52 [00:14&lt;00:00,  3.31it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nEpoch 13:  96%|█████████▌| 50/52 [00:14&lt;00:00,  3.35it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nEpoch 13:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.39it/s, loss=0.538, v_num=0, val_acc=0.761, val_loss=0.931, train_acc=0.845, train_loss=0.63]\nEpoch 00013: val_checkpoint_on  was not in top 1\nEpoch 13: 100%|██████████| 52/52 [00:15&lt;00:00,  3.45it/s, loss=0.538, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.845, train_loss=0.63] \nEpoch 14:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.90it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 14:  81%|████████  | 42/52 [00:14&lt;00:03,  2.94it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nEpoch 14:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.99it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nEpoch 14:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.03it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nEpoch 14:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.07it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nEpoch 14:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.11it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nEpoch 14:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.15it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nEpoch 14:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.19it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nEpoch 14:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.23it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nEpoch 14:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.27it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nEpoch 14:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.31it/s, loss=0.473, v_num=0, val_acc=0.737, val_loss=1.03, train_acc=0.866, train_loss=0.536]\nEpoch 00014: val_checkpoint_on  was not in top 1\nEpoch 14: 100%|██████████| 52/52 [00:15&lt;00:00,  3.37it/s, loss=0.473, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.866, train_loss=0.536]\nEpoch 15:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.87it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 15:  81%|████████  | 42/52 [00:14&lt;00:03,  2.91it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nEpoch 15:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.96it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nEpoch 15:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.00it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nEpoch 15:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.04it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nEpoch 15:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.08it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nEpoch 15:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.12it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nEpoch 15:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.16it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nEpoch 15:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.20it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nEpoch 15:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.23it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nEpoch 15:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.27it/s, loss=0.425, v_num=0, val_acc=0.787, val_loss=0.943, train_acc=0.887, train_loss=0.48]\nEpoch 00015: val_checkpoint_on  was not in top 1\nEpoch 15: 100%|██████████| 52/52 [00:15&lt;00:00,  3.33it/s, loss=0.425, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.887, train_loss=0.48]\nEpoch 16:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.86it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 16:  81%|████████  | 42/52 [00:14&lt;00:03,  2.91it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nEpoch 16:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.95it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nEpoch 16:  85%|████████▍ | 44/52 [00:14&lt;00:02,  2.99it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nEpoch 16:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.03it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nEpoch 16:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.07it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nEpoch 16:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.11it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nEpoch 16:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.15it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nEpoch 16:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.19it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nEpoch 16:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.23it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nEpoch 16:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.27it/s, loss=0.385, v_num=0, val_acc=0.807, val_loss=0.934, train_acc=0.89, train_loss=0.453]\nEpoch 00016: val_checkpoint_on  was not in top 1\nEpoch 16: 100%|██████████| 52/52 [00:15&lt;00:00,  3.33it/s, loss=0.385, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.89, train_loss=0.453] \nEpoch 17:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.90it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 17:  81%|████████  | 42/52 [00:14&lt;00:03,  2.94it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nEpoch 17:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.99it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nEpoch 17:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.03it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nEpoch 17:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.07it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nEpoch 17:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.11it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nEpoch 17:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.15it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nEpoch 17:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.19it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nEpoch 17:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.22it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nEpoch 17:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.26it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nEpoch 17:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.30it/s, loss=0.380, v_num=0, val_acc=0.791, val_loss=1.01, train_acc=0.912, train_loss=0.381]\nEpoch 00017: val_checkpoint_on  was not in top 1\nEpoch 17: 100%|██████████| 52/52 [00:15&lt;00:00,  3.36it/s, loss=0.380, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.912, train_loss=0.381]\nEpoch 18:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.84it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 18:  81%|████████  | 42/52 [00:14&lt;00:03,  2.88it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nEpoch 18:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.93it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nEpoch 18:  85%|████████▍ | 44/52 [00:14&lt;00:02,  2.97it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nEpoch 18:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.01it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nEpoch 18:  88%|████████▊ | 46/52 [00:15&lt;00:01,  3.05it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nEpoch 18:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.09it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nEpoch 18:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.13it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nEpoch 18:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.17it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nEpoch 18:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.21it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nEpoch 18:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.24it/s, loss=0.224, v_num=0, val_acc=0.782, val_loss=1.05, train_acc=0.915, train_loss=0.352]\nEpoch 00018: val_checkpoint_on reached 0.88701 (best 0.88701), saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_18.ckpt as top 1\nEpoch 18: 100%|██████████| 52/52 [00:28&lt;00:00,  1.81it/s, loss=0.224, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.915, train_loss=0.352]\nEpoch 19:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.88it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 19:  81%|████████  | 42/52 [00:14&lt;00:03,  2.92it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nEpoch 19:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.96it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nEpoch 19:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.00it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nEpoch 19:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.05it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nEpoch 19:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.09it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nEpoch 19:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.13it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nEpoch 19:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.17it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nEpoch 19:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.21it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nEpoch 19:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.24it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nEpoch 19:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.28it/s, loss=0.232, v_num=0, val_acc=0.801, val_loss=0.887, train_acc=0.943, train_loss=0.281]\nEpoch 00019: val_checkpoint_on  was not in top 1\nEpoch 19: 100%|██████████| 52/52 [00:15&lt;00:00,  3.34it/s, loss=0.232, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.943, train_loss=0.281]\nEpoch 20:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.86it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 20:  81%|████████  | 42/52 [00:14&lt;00:03,  2.91it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nEpoch 20:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.95it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nEpoch 20:  85%|████████▍ | 44/52 [00:14&lt;00:02,  2.99it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nEpoch 20:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.03it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nEpoch 20:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.07it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nEpoch 20:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.12it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nEpoch 20:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.15it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nEpoch 20:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.19it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nEpoch 20:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.23it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nEpoch 20:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.27it/s, loss=0.224, v_num=0, val_acc=0.795, val_loss=0.951, train_acc=0.949, train_loss=0.249]\nEpoch 00020: val_checkpoint_on  was not in top 1\nEpoch 20: 100%|██████████| 52/52 [00:15&lt;00:00,  3.33it/s, loss=0.224, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.249] \nEpoch 21:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.84it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 21:  81%|████████  | 42/52 [00:14&lt;00:03,  2.88it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nEpoch 21:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.92it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nEpoch 21:  85%|████████▍ | 44/52 [00:14&lt;00:02,  2.96it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nEpoch 21:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.01it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nEpoch 21:  88%|████████▊ | 46/52 [00:15&lt;00:01,  3.05it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nEpoch 21:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.09it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nEpoch 21:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.13it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nEpoch 21:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.17it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nEpoch 21:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.21it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nEpoch 21:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.24it/s, loss=0.267, v_num=0, val_acc=0.768, val_loss=1.17, train_acc=0.949, train_loss=0.236]\nEpoch 00021: val_checkpoint_on  was not in top 1\nEpoch 21: 100%|██████████| 52/52 [00:15&lt;00:00,  3.30it/s, loss=0.267, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.949, train_loss=0.236]\nEpoch 22:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.88it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 22:  81%|████████  | 42/52 [00:14&lt;00:03,  2.93it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nEpoch 22:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.97it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nEpoch 22:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.01it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nEpoch 22:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.05it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nEpoch 22:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.09it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nEpoch 22:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.14it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nEpoch 22:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.18it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nEpoch 22:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.22it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nEpoch 22:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.26it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nEpoch 22:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.29it/s, loss=0.203, v_num=0, val_acc=0.804, val_loss=0.979, train_acc=0.942, train_loss=0.221]\nEpoch 00022: val_checkpoint_on  was not in top 1\nEpoch 22: 100%|██████████| 52/52 [00:15&lt;00:00,  3.35it/s, loss=0.203, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.942, train_loss=0.221]\nEpoch 23:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.90it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 23:  81%|████████  | 42/52 [00:14&lt;00:03,  2.95it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nEpoch 23:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.99it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nEpoch 23:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.03it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nEpoch 23:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.07it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nEpoch 23:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.12it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nEpoch 23:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.16it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nEpoch 23:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.20it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nEpoch 23:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.24it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nEpoch 23:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.28it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nEpoch 23:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.31it/s, loss=0.179, v_num=0, val_acc=0.795, val_loss=0.926, train_acc=0.962, train_loss=0.198]\nEpoch 00023: val_checkpoint_on  was not in top 1\nEpoch 23: 100%|██████████| 52/52 [00:15&lt;00:00,  3.37it/s, loss=0.179, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.198] \nEpoch 24:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.90it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 24:  81%|████████  | 42/52 [00:14&lt;00:03,  2.94it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nEpoch 24:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.99it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nEpoch 24:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.03it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nEpoch 24:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.07it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nEpoch 24:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.11it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nEpoch 24:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.15it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nEpoch 24:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.19it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nEpoch 24:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.23it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nEpoch 24:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.27it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nEpoch 24:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.31it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.11, train_acc=0.962, train_loss=0.185]\nEpoch 00024: val_checkpoint_on  was not in top 1\nEpoch 24: 100%|██████████| 52/52 [00:15&lt;00:00,  3.37it/s, loss=0.169, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.962, train_loss=0.185]\nEpoch 25:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.89it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 25:  81%|████████  | 42/52 [00:14&lt;00:03,  2.94it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nEpoch 25:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.98it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nEpoch 25:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.02it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nEpoch 25:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.06it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nEpoch 25:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.11it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nEpoch 25:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.15it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nEpoch 25:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.19it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nEpoch 25:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.23it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nEpoch 25:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.27it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nEpoch 25:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.30it/s, loss=0.161, v_num=0, val_acc=0.765, val_loss=1.02, train_acc=0.961, train_loss=0.169]\nEpoch 00025: val_checkpoint_on  was not in top 1\nEpoch 25: 100%|██████████| 52/52 [00:15&lt;00:00,  3.36it/s, loss=0.161, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.961, train_loss=0.169]\nEpoch 26:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.91it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 26:  81%|████████  | 42/52 [00:14&lt;00:03,  2.95it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nEpoch 26:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.99it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nEpoch 26:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.04it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nEpoch 26:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.08it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nEpoch 26:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.12it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nEpoch 26:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.16it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nEpoch 26:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.20it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nEpoch 26:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.24it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nEpoch 26:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.28it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nEpoch 26:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.32it/s, loss=0.131, v_num=0, val_acc=0.741, val_loss=1.57, train_acc=0.972, train_loss=0.142]\nEpoch 00026: val_checkpoint_on  was not in top 1\nEpoch 26: 100%|██████████| 52/52 [00:15&lt;00:00,  3.38it/s, loss=0.131, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.972, train_loss=0.142]\nEpoch 27:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.90it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 27:  81%|████████  | 42/52 [00:14&lt;00:03,  2.94it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nEpoch 27:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.98it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nEpoch 27:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.03it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nEpoch 27:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.07it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nEpoch 27:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.11it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nEpoch 27:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.15it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nEpoch 27:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.19it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nEpoch 27:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.23it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nEpoch 27:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.27it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nEpoch 27:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.31it/s, loss=0.098, v_num=0, val_acc=0.807, val_loss=0.997, train_acc=0.973, train_loss=0.137]\nEpoch 00027: val_checkpoint_on  was not in top 1\nEpoch 27: 100%|██████████| 52/52 [00:15&lt;00:00,  3.37it/s, loss=0.098, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.973, train_loss=0.137] \nEpoch 28:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.88it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 28:  81%|████████  | 42/52 [00:14&lt;00:03,  2.93it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nEpoch 28:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.97it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nEpoch 28:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.01it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nEpoch 28:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.05it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nEpoch 28:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.09it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nEpoch 28:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.13it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nEpoch 28:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.17it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nEpoch 28:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.21it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nEpoch 28:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.25it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nEpoch 28:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.28it/s, loss=0.090, v_num=0, val_acc=0.782, val_loss=1.14, train_acc=0.977, train_loss=0.113]\nEpoch 00028: val_checkpoint_on  was not in top 1\nEpoch 28: 100%|██████████| 52/52 [00:15&lt;00:00,  3.34it/s, loss=0.090, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.977, train_loss=0.113]\nEpoch 29:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.80it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 29:  81%|████████  | 42/52 [00:14&lt;00:03,  2.84it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nEpoch 29:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.88it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nEpoch 29:  85%|████████▍ | 44/52 [00:15&lt;00:02,  2.93it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nEpoch 29:  87%|████████▋ | 45/52 [00:15&lt;00:02,  2.97it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nEpoch 29:  88%|████████▊ | 46/52 [00:15&lt;00:01,  3.01it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nEpoch 29:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.05it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nEpoch 29:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.09it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nEpoch 29:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.13it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nEpoch 29:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.16it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nEpoch 29:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.20it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.15, train_acc=0.989, train_loss=0.0809]\nEpoch 00029: val_checkpoint_on  was not in top 1\nEpoch 29: 100%|██████████| 52/52 [00:15&lt;00:00,  3.26it/s, loss=0.084, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.989, train_loss=0.0809]\nEpoch 30:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.89it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 30:  81%|████████  | 42/52 [00:14&lt;00:03,  2.93it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nEpoch 30:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.98it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nEpoch 30:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.02it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nEpoch 30:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.06it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nEpoch 30:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.10it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nEpoch 30:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.14it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nEpoch 30:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.18it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nEpoch 30:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.22it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nEpoch 30:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.26it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nEpoch 30:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.30it/s, loss=0.119, v_num=0, val_acc=0.777, val_loss=1.28, train_acc=0.986, train_loss=0.0777]\nEpoch 00030: val_checkpoint_on  was not in top 1\nEpoch 30: 100%|██████████| 52/52 [00:15&lt;00:00,  3.36it/s, loss=0.119, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.986, train_loss=0.0777]\nEpoch 31:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.91it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 31:  81%|████████  | 42/52 [00:14&lt;00:03,  2.95it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nEpoch 31:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.99it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nEpoch 31:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.04it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nEpoch 31:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.08it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nEpoch 31:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.12it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nEpoch 31:  90%|█████████ | 47/52 [00:14&lt;00:01,  3.16it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nEpoch 31:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.20it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nEpoch 31:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.24it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nEpoch 31:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.28it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nEpoch 31:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.32it/s, loss=0.168, v_num=0, val_acc=0.784, val_loss=1.14, train_acc=0.976, train_loss=0.101]\nEpoch 00031: val_checkpoint_on  was not in top 1\nEpoch 31: 100%|██████████| 52/52 [00:15&lt;00:00,  3.38it/s, loss=0.168, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.976, train_loss=0.101] \nEpoch 32:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.85it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 32:  81%|████████  | 42/52 [00:14&lt;00:03,  2.89it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nEpoch 32:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.93it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nEpoch 32:  85%|████████▍ | 44/52 [00:14&lt;00:02,  2.97it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nEpoch 32:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.01it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nEpoch 32:  88%|████████▊ | 46/52 [00:15&lt;00:01,  3.05it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nEpoch 32:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.09it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nEpoch 32:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.13it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nEpoch 32:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.16it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nEpoch 32:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.20it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nEpoch 32:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.24it/s, loss=0.101, v_num=0, val_acc=0.79, val_loss=1.13, train_acc=0.959, train_loss=0.165]\nEpoch 00032: val_checkpoint_on  was not in top 1\nEpoch 32: 100%|██████████| 52/52 [00:15&lt;00:00,  3.30it/s, loss=0.101, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.959, train_loss=0.165]\nEpoch 33:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.81it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 33:  81%|████████  | 42/52 [00:14&lt;00:03,  2.86it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nEpoch 33:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.90it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nEpoch 33:  85%|████████▍ | 44/52 [00:14&lt;00:02,  2.94it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nEpoch 33:  87%|████████▋ | 45/52 [00:15&lt;00:02,  2.98it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nEpoch 33:  88%|████████▊ | 46/52 [00:15&lt;00:01,  3.02it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nEpoch 33:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.06it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nEpoch 33:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.09it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nEpoch 33:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.13it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nEpoch 33:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.16it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nEpoch 33:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.20it/s, loss=0.064, v_num=0, val_acc=0.768, val_loss=1.03, train_acc=0.979, train_loss=0.0938]\nEpoch 00033: val_checkpoint_on  was not in top 1\nEpoch 33: 100%|██████████| 52/52 [00:15&lt;00:00,  3.26it/s, loss=0.064, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.979, train_loss=0.0938]\nEpoch 34:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.85it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 34:  81%|████████  | 42/52 [00:14&lt;00:03,  2.89it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nEpoch 34:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.93it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nEpoch 34:  85%|████████▍ | 44/52 [00:14&lt;00:02,  2.97it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nEpoch 34:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.01it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nEpoch 34:  88%|████████▊ | 46/52 [00:15&lt;00:01,  3.05it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nEpoch 34:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.09it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nEpoch 34:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.13it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nEpoch 34:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.16it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nEpoch 34:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.20it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nEpoch 34:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.23it/s, loss=0.090, v_num=0, val_acc=0.804, val_loss=0.987, train_acc=0.989, train_loss=0.0667]\nEpoch 00034: val_checkpoint_on  was not in top 1\nEpoch 34: 100%|██████████| 52/52 [00:15&lt;00:00,  3.29it/s, loss=0.090, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.989, train_loss=0.0667] \nEpoch 35:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.88it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 35:  81%|████████  | 42/52 [00:14&lt;00:03,  2.92it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nEpoch 35:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.96it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nEpoch 35:  85%|████████▍ | 44/52 [00:14&lt;00:02,  3.00it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nEpoch 35:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.04it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nEpoch 35:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.08it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nEpoch 35:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.12it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nEpoch 35:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.16it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nEpoch 35:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.20it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nEpoch 35:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.23it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nEpoch 35:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.27it/s, loss=0.089, v_num=0, val_acc=0.743, val_loss=1.09, train_acc=0.984, train_loss=0.0731]\nEpoch 00035: val_checkpoint_on  was not in top 1\nEpoch 35: 100%|██████████| 52/52 [00:15&lt;00:00,  3.33it/s, loss=0.089, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.984, train_loss=0.0731]   \nEpoch 36:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.80it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 36:  81%|████████  | 42/52 [00:14&lt;00:03,  2.84it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nEpoch 36:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.88it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nEpoch 36:  85%|████████▍ | 44/52 [00:15&lt;00:02,  2.92it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nEpoch 36:  87%|████████▋ | 45/52 [00:15&lt;00:02,  2.96it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nEpoch 36:  88%|████████▊ | 46/52 [00:15&lt;00:01,  3.00it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nEpoch 36:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.04it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nEpoch 36:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.08it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nEpoch 36:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.12it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nEpoch 36:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.16it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nEpoch 36:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.19it/s, loss=0.082, v_num=0, val_acc=0.801, val_loss=1, train_acc=0.98, train_loss=0.0763]\nEpoch 00036: val_checkpoint_on  was not in top 1\nEpoch 36: 100%|██████████| 52/52 [00:15&lt;00:00,  3.25it/s, loss=0.082, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0763]\nEpoch 37:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.84it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 37:  81%|████████  | 42/52 [00:14&lt;00:03,  2.88it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nEpoch 37:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.93it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nEpoch 37:  85%|████████▍ | 44/52 [00:14&lt;00:02,  2.97it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nEpoch 37:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.01it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nEpoch 37:  88%|████████▊ | 46/52 [00:15&lt;00:01,  3.05it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nEpoch 37:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.09it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nEpoch 37:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.13it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nEpoch 37:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.17it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nEpoch 37:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.21it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nEpoch 37:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.24it/s, loss=0.050, v_num=0, val_acc=0.81, val_loss=1.04, train_acc=0.98, train_loss=0.0874]\nEpoch 00037: val_checkpoint_on  was not in top 1\nEpoch 37: 100%|██████████| 52/52 [00:15&lt;00:00,  3.30it/s, loss=0.050, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.98, train_loss=0.0874]\nEpoch 38:  79%|███████▉  | 41/52 [00:14&lt;00:03,  2.86it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 38:  81%|████████  | 42/52 [00:14&lt;00:03,  2.90it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nEpoch 38:  83%|████████▎ | 43/52 [00:14&lt;00:03,  2.95it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nEpoch 38:  85%|████████▍ | 44/52 [00:14&lt;00:02,  2.99it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nEpoch 38:  87%|████████▋ | 45/52 [00:14&lt;00:02,  3.03it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nEpoch 38:  88%|████████▊ | 46/52 [00:14&lt;00:01,  3.07it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nEpoch 38:  90%|█████████ | 47/52 [00:15&lt;00:01,  3.11it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nEpoch 38:  92%|█████████▏| 48/52 [00:15&lt;00:01,  3.15it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nEpoch 38:  94%|█████████▍| 49/52 [00:15&lt;00:00,  3.19it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nEpoch 38:  96%|█████████▌| 50/52 [00:15&lt;00:00,  3.23it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nEpoch 38:  98%|█████████▊| 51/52 [00:15&lt;00:00,  3.26it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.35, train_acc=0.989, train_loss=0.0637]\nEpoch 00038: val_checkpoint_on  was not in top 1\nEpoch 38: 100%|██████████| 52/52 [00:15&lt;00:00,  3.32it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.21, train_acc=0.989, train_loss=0.0637]\n                                                           \u001b[ASaving latest checkpoint..\nEpoch 00039: early stopping triggered.\nEpoch 38: 100%|██████████| 52/52 [00:15&lt;00:00,  3.32it/s, loss=0.067, v_num=0, val_acc=0.777, val_loss=1.21, train_acc=0.989, train_loss=0.0637]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, early_stop_callback=early_stop_callback, checkpoint_callback=checkpoint_callback, max_epochs=EPOCHS)\n",
    "trainer.fit(roberta, corefx_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(0.8870, device=&#39;cuda:0&#39;)\n"
    }
   ],
   "source": [
    "print(checkpoint_callback.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "d:\\work\\RobertaGithubIssuesClassification\\Models\\pl\\_ckpt_epoch_18.ckpt\n"
    }
   ],
   "source": [
    "print(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "source": [
    "# Test on new data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;, &#39;roberta.pooler.dense.weight&#39;, &#39;roberta.pooler.dense.bias&#39;]\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;]\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CorefxRobertaModel(\n  (roberta_model): RobertaForSequenceClassification(\n    (roberta): RobertaModel(\n      (embeddings): RobertaEmbeddings(\n        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n        (position_embeddings): Embedding(514, 768, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.3, inplace=False)\n      )\n      (encoder): RobertaEncoder(\n        (layer): ModuleList(\n          (0): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (1): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (2): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (3): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (4): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (5): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (6): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (7): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (8): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (9): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (10): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (11): RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.3, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (classifier): RobertaClassificationHead(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (dropout): Dropout(p=0.3, inplace=False)\n      (out_proj): Linear(in_features=768, out_features=22, bias=True)\n    )\n  )\n)"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "roberta = CorefxRobertaModel.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "roberta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = \"AppDomain.SetPrincipalPolicy(PrincipalPolicy.WindowsPrincipal) works only once. Setting the PrincipalPolicy on the current AppDomain to WindowsPrincipal works only for the first thread being started. Any subsequent thread has Thread.CurrentPrincipal evaluated to NULL.\"\n",
    "label = \"area-System.Security\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_pt = roberta_tokenizer(issue, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt', add_special_tokens=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.1868, -1.4769, -0.4210,  3.5745,  1.4349,  0.5693,  0.1789, -0.2447,\n         -0.5479, -0.5767, -2.3285, -0.5988, -1.0072, -1.5970, -0.9118,  4.4835,\n         -0.5231, -1.3572, -0.5189, -0.6711, -0.9153, -0.4658]],\n       grad_fn=&lt;AddmmBackward&gt;), hidden_states=None, attentions=None)\n"
    }
   ],
   "source": [
    "pt_result = roberta(encoded_pt[\"input_ids\"], encoded_pt[\"attention_mask\"])\n",
    "print(pt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "index : 15, category : area-System.Data\n"
    }
   ],
   "source": [
    "index = np.argmax(pt_result[0].detach().numpy())\n",
    "print(f\"index : {index}, category : {lookup[str(index)]}\")\n",
    "# not what was expected ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.functional.softmax(pt_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "area-System.Net             : 2.31% confidence\narea-Infrastructure         : 0.16% confidence\narea-System.ComponentModel  : 0.46% confidence\narea-System.Security        : 25.11% confidence\narea-System.Runtime         : 2.96% confidence\narea-System.IO              : 1.24% confidence\narea-System.Xml             : 0.84% confidence\narea-System.Collections     : 0.55% confidence\narea-System.Threading       : 0.41% confidence\narea-System.Reflection      : 0.40% confidence\narea-System.Memory          : 0.07% confidence\narea-System.Diagnostics     : 0.39% confidence\narea-Serialization          : 0.26% confidence\narea-System.Drawing         : 0.14% confidence\narea-Meta                   : 0.28% confidence\narea-System.Data            : 62.33% confidence\narea-Microsoft.CSharp       : 0.42% confidence\narea-System.Numerics        : 0.18% confidence\narea-System.Text            : 0.42% confidence\narea-System.Globalization   : 0.36% confidence\narea-System.Linq            : 0.28% confidence\narea-System.Console         : 0.44% confidence\n"
    }
   ],
   "source": [
    "for i, value in enumerate(softmax[0]):\n",
    "    print(f\"{lookup[str(i)] : <27} : {value.item() * 100 :.2f}% confidence\")"
   ]
  },
  {
   "source": [
    "# Export to ONNX"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(roberta,               # model being run\n",
    "                  (encoded_pt[\"input_ids\"], encoded_pt[\"attention_mask\"]),  # model input (or a tuple for multiple inputs)\n",
    "                  os.path.join(MODELS_FOLDER, \"pl\", \"pl_roberta_github_issues.onnx\"),   # where to save the model\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=12,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input_ids', 'attention_mask'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input_ids' : {0 : 'batch_size'},\n",
    "                                'attention_mask' : {0 : 'batch_size'},\n",
    "                                'output' : {0 : 'batch_size'}}\n",
    "                    )"
   ]
  }
 ]
}