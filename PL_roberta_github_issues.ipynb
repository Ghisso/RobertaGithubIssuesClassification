{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit ('base': conda)",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "import json\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(\"./\", \"Data\")\n",
    "MODELS_PATH = os.path.join(\"./\", \"Models\")\n",
    "TRAIN_FILE_NAME = \"corefx_cleaned_train.csv\"\n",
    "VAL_FILE_NAME = \"corefx_cleaned_val.csv\"\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_FOLDER, TRAIN_FILE_NAME)\n",
    "VAL_FILE_PATH = os.path.join(DATA_FOLDER, VAL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorefxDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, csv_path: str, tokenizer, max_len: int = 128):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = tokenizer(self.df[\"Text\"].iloc[idx], padding='max_length', truncation=True, max_length=self.max_len)\n",
    "        label = self.df[\"Label\"].iloc[idx]\n",
    "        return ({\n",
    "            'input_ids' : torch.tensor(encoded['input_ids'], dtype=torch.long),\n",
    "            'attention_mask' : torch.tensor(encoded['attention_mask'], dtype=torch.long)},\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorefxIssuesDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, data_path: str, tokenizer, max_len: int = 128, batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_split = CorefxDataset(os.path.join(self.data_path, \"corefx_cleaned_train.csv\"), self.tokenizer, self.max_len)\n",
    "        return torch.utils.data.DataLoader(train_split, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_split = CorefxDataset(os.path.join(self.data_path, \"corefx_cleaned_val.csv\"), self.tokenizer, self.max_len)\n",
    "        return torch.utils.data.DataLoader(val_split, batch_size=self.batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorefxModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(input_ids = x['input_ids'], attention_mask = x['attention_mask'])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        outputs = self(x)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs.logits, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        outputs = self(x)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs.logits, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "corefx_dm = CorefxIssuesDataModule(DATA_FOLDER, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, 'lookup.json')) as json_file: \n",
    "    lookup = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(lookup.keys()) // 2, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = CorefxModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "GPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\n\n  | Name  | Type                             | Params\n-----------------------------------------------------------\n0 | model | RobertaForSequenceClassification | 125 M \nEpoch 0:  79%|███████▉  | 41/52 [00:08<00:02,  4.69it/s, loss=2.284, v_num=13]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 0:  85%|████████▍ | 44/52 [00:08<00:01,  4.96it/s, loss=2.284, v_num=13]\nEpoch 0:  90%|█████████ | 47/52 [00:09<00:00,  5.21it/s, loss=2.284, v_num=13]\nEpoch 0:  96%|█████████▌| 50/52 [00:09<00:00,  5.48it/s, loss=2.284, v_num=13]\nEpoch 0: 100%|██████████| 52/52 [00:09<00:00,  5.63it/s, loss=2.284, v_num=13]\nEpoch 1:  79%|███████▉  | 41/52 [00:08<00:02,  4.73it/s, loss=1.693, v_num=13]\nEpoch 1:  81%|████████  | 42/52 [00:08<00:02,  4.82it/s, loss=1.693, v_num=13]\nEpoch 1:  87%|████████▋ | 45/52 [00:08<00:01,  5.09it/s, loss=1.693, v_num=13]\nEpoch 1:  92%|█████████▏| 48/52 [00:08<00:00,  5.36it/s, loss=1.693, v_num=13]\nValidating:  73%|███████▎  | 8/11 [00:00<00:00, 22.72it/s]\u001b[A\nEpoch 1: 100%|██████████| 52/52 [00:09<00:00,  5.68it/s, loss=1.693, v_num=13]\nEpoch 2:  79%|███████▉  | 41/52 [00:08<00:02,  4.80it/s, loss=1.105, v_num=13]\nEpoch 2:  81%|████████  | 42/52 [00:08<00:02,  4.89it/s, loss=1.105, v_num=13]\nEpoch 2:  87%|████████▋ | 45/52 [00:08<00:01,  5.15it/s, loss=1.105, v_num=13]\nEpoch 2:  92%|█████████▏| 48/52 [00:08<00:00,  5.42it/s, loss=1.105, v_num=13]\nEpoch 2: 100%|██████████| 52/52 [00:09<00:00,  5.76it/s, loss=1.105, v_num=13]\nEpoch 3:  79%|███████▉  | 41/52 [00:08<00:02,  4.74it/s, loss=0.824, v_num=13]\nEpoch 3:  81%|████████  | 42/52 [00:08<00:02,  4.83it/s, loss=0.824, v_num=13]\nEpoch 3:  87%|████████▋ | 45/52 [00:08<00:01,  5.11it/s, loss=0.824, v_num=13]\nEpoch 3:  92%|█████████▏| 48/52 [00:08<00:00,  5.38it/s, loss=0.824, v_num=13]\nEpoch 3:  98%|█████████▊| 51/52 [00:09<00:00,  5.62it/s, loss=0.824, v_num=13]\nEpoch 3: 100%|██████████| 52/52 [00:09<00:00,  5.69it/s, loss=0.824, v_num=13]\nEpoch 4:  79%|███████▉  | 41/52 [00:08<00:02,  4.72it/s, loss=0.570, v_num=13]\nEpoch 4:  81%|████████  | 42/52 [00:08<00:02,  4.81it/s, loss=0.570, v_num=13]\nEpoch 4:  87%|████████▋ | 45/52 [00:08<00:01,  5.09it/s, loss=0.570, v_num=13]\nEpoch 4:  92%|█████████▏| 48/52 [00:08<00:00,  5.36it/s, loss=0.570, v_num=13]\nEpoch 4:  98%|█████████▊| 51/52 [00:09<00:00,  5.60it/s, loss=0.570, v_num=13]\nEpoch 4: 100%|██████████| 52/52 [00:09<00:00,  5.66it/s, loss=0.570, v_num=13]\nEpoch 5:  79%|███████▉  | 41/52 [00:08<00:02,  4.71it/s, loss=0.431, v_num=13]\nEpoch 5:  81%|████████  | 42/52 [00:08<00:02,  4.80it/s, loss=0.431, v_num=13]\nEpoch 5:  87%|████████▋ | 45/52 [00:08<00:01,  5.08it/s, loss=0.431, v_num=13]\nEpoch 5:  92%|█████████▏| 48/52 [00:08<00:00,  5.34it/s, loss=0.431, v_num=13]\nEpoch 5: 100%|██████████| 52/52 [00:09<00:00,  5.66it/s, loss=0.431, v_num=13]\nEpoch 6:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.350, v_num=13]\nEpoch 6:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.350, v_num=13]\nEpoch 6:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.350, v_num=13]\nEpoch 6:  92%|█████████▏| 48/52 [00:09<00:00,  5.31it/s, loss=0.350, v_num=13]\nEpoch 6:  98%|█████████▊| 51/52 [00:09<00:00,  5.55it/s, loss=0.350, v_num=13]\nEpoch 6: 100%|██████████| 52/52 [00:09<00:00,  5.62it/s, loss=0.350, v_num=13]\nEpoch 7:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.210, v_num=13]\nEpoch 7:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.210, v_num=13]\nEpoch 7:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.210, v_num=13]\nEpoch 7:  92%|█████████▏| 48/52 [00:09<00:00,  5.31it/s, loss=0.210, v_num=13]\nEpoch 7: 100%|██████████| 52/52 [00:09<00:00,  5.62it/s, loss=0.210, v_num=13]\nEpoch 8:  79%|███████▉  | 41/52 [00:08<00:02,  4.70it/s, loss=0.176, v_num=13]\nEpoch 8:  81%|████████  | 42/52 [00:08<00:02,  4.78it/s, loss=0.176, v_num=13]\nEpoch 8:  87%|████████▋ | 45/52 [00:08<00:01,  5.05it/s, loss=0.176, v_num=13]\nEpoch 8:  92%|█████████▏| 48/52 [00:09<00:00,  5.32it/s, loss=0.176, v_num=13]\nEpoch 8:  98%|█████████▊| 51/52 [00:09<00:00,  5.56it/s, loss=0.176, v_num=13]\nEpoch 8: 100%|██████████| 52/52 [00:09<00:00,  5.63it/s, loss=0.176, v_num=13]\nEpoch 9:  79%|███████▉  | 41/52 [00:08<00:02,  4.71it/s, loss=0.115, v_num=13]\nEpoch 9:  81%|████████  | 42/52 [00:08<00:02,  4.79it/s, loss=0.115, v_num=13]\nEpoch 9:  87%|████████▋ | 45/52 [00:08<00:01,  5.06it/s, loss=0.115, v_num=13]\nEpoch 9:  92%|█████████▏| 48/52 [00:08<00:00,  5.33it/s, loss=0.115, v_num=13]\nEpoch 9:  98%|█████████▊| 51/52 [00:09<00:00,  5.57it/s, loss=0.115, v_num=13]\nEpoch 9: 100%|██████████| 52/52 [00:09<00:00,  5.64it/s, loss=0.115, v_num=13]\nEpoch 10:  79%|███████▉  | 41/52 [00:08<00:02,  4.71it/s, loss=0.080, v_num=13]\nEpoch 10:  81%|████████  | 42/52 [00:08<00:02,  4.80it/s, loss=0.080, v_num=13]\nEpoch 10:  87%|████████▋ | 45/52 [00:08<00:01,  5.08it/s, loss=0.080, v_num=13]\nEpoch 10:  92%|█████████▏| 48/52 [00:08<00:00,  5.34it/s, loss=0.080, v_num=13]\nEpoch 10: 100%|██████████| 52/52 [00:09<00:00,  5.65it/s, loss=0.080, v_num=13]\nEpoch 11:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.066, v_num=13]\nEpoch 11:  81%|████████  | 42/52 [00:08<00:02,  4.76it/s, loss=0.066, v_num=13]\nEpoch 11:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.066, v_num=13]\nEpoch 11:  92%|█████████▏| 48/52 [00:09<00:00,  5.31it/s, loss=0.066, v_num=13]\nEpoch 11: 100%|██████████| 52/52 [00:09<00:00,  5.62it/s, loss=0.066, v_num=13]\nEpoch 12:  79%|███████▉  | 41/52 [00:08<00:02,  4.70it/s, loss=0.048, v_num=13]\nEpoch 12:  81%|████████  | 42/52 [00:08<00:02,  4.79it/s, loss=0.048, v_num=13]\nEpoch 12:  87%|████████▋ | 45/52 [00:08<00:01,  5.05it/s, loss=0.048, v_num=13]\nEpoch 12:  92%|█████████▏| 48/52 [00:09<00:00,  5.32it/s, loss=0.048, v_num=13]\nEpoch 12:  98%|█████████▊| 51/52 [00:09<00:00,  5.56it/s, loss=0.048, v_num=13]\nEpoch 12: 100%|██████████| 52/52 [00:09<00:00,  5.63it/s, loss=0.048, v_num=13]\nEpoch 13:  79%|███████▉  | 41/52 [00:08<00:02,  4.70it/s, loss=0.042, v_num=13]\nEpoch 13:  81%|████████  | 42/52 [00:08<00:02,  4.79it/s, loss=0.042, v_num=13]\nEpoch 13:  87%|████████▋ | 45/52 [00:08<00:01,  5.06it/s, loss=0.042, v_num=13]\nEpoch 13:  92%|█████████▏| 48/52 [00:09<00:00,  5.33it/s, loss=0.042, v_num=13]\nEpoch 13:  98%|█████████▊| 51/52 [00:09<00:00,  5.57it/s, loss=0.042, v_num=13]\nEpoch 13: 100%|██████████| 52/52 [00:09<00:00,  5.64it/s, loss=0.042, v_num=13]\nEpoch 14:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.048, v_num=13]\nEpoch 14:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.048, v_num=13]\nEpoch 14:  87%|████████▋ | 45/52 [00:08<00:01,  5.03it/s, loss=0.048, v_num=13]\nEpoch 14:  92%|█████████▏| 48/52 [00:09<00:00,  5.29it/s, loss=0.048, v_num=13]\nEpoch 14:  98%|█████████▊| 51/52 [00:09<00:00,  5.52it/s, loss=0.048, v_num=13]\nEpoch 14: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.048, v_num=13]\nEpoch 15:  79%|███████▉  | 41/52 [00:08<00:02,  4.67it/s, loss=0.035, v_num=13]\nEpoch 15:  81%|████████  | 42/52 [00:08<00:02,  4.76it/s, loss=0.035, v_num=13]\nEpoch 15:  87%|████████▋ | 45/52 [00:08<00:01,  5.03it/s, loss=0.035, v_num=13]\nEpoch 15:  92%|█████████▏| 48/52 [00:09<00:00,  5.29it/s, loss=0.035, v_num=13]\nEpoch 15:  98%|█████████▊| 51/52 [00:09<00:00,  5.53it/s, loss=0.035, v_num=13]\nEpoch 15: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.035, v_num=13]\nEpoch 16:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.028, v_num=13]\nEpoch 16:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.028, v_num=13]\nEpoch 16:  87%|████████▋ | 45/52 [00:08<00:01,  5.01it/s, loss=0.028, v_num=13]\nEpoch 16:  92%|█████████▏| 48/52 [00:09<00:00,  5.28it/s, loss=0.028, v_num=13]\nEpoch 16:  98%|█████████▊| 51/52 [00:09<00:00,  5.52it/s, loss=0.028, v_num=13]\nEpoch 16: 100%|██████████| 52/52 [00:09<00:00,  5.59it/s, loss=0.028, v_num=13]\nEpoch 17:  79%|███████▉  | 41/52 [00:08<00:02,  4.69it/s, loss=0.020, v_num=13]\nEpoch 17:  81%|████████  | 42/52 [00:08<00:02,  4.78it/s, loss=0.020, v_num=13]\nEpoch 17:  87%|████████▋ | 45/52 [00:08<00:01,  5.05it/s, loss=0.020, v_num=13]\nEpoch 17:  92%|█████████▏| 48/52 [00:09<00:00,  5.32it/s, loss=0.020, v_num=13]\nEpoch 17: 100%|██████████| 52/52 [00:09<00:00,  5.63it/s, loss=0.020, v_num=13]\nEpoch 18:  79%|███████▉  | 41/52 [00:08<00:02,  4.70it/s, loss=0.015, v_num=13]\nEpoch 18:  81%|████████  | 42/52 [00:08<00:02,  4.79it/s, loss=0.015, v_num=13]\nEpoch 18:  87%|████████▋ | 45/52 [00:08<00:01,  5.07it/s, loss=0.015, v_num=13]\nEpoch 18:  92%|█████████▏| 48/52 [00:09<00:00,  5.33it/s, loss=0.015, v_num=13]\nEpoch 18:  98%|█████████▊| 51/52 [00:09<00:00,  5.56it/s, loss=0.015, v_num=13]\nEpoch 18: 100%|██████████| 52/52 [00:09<00:00,  5.63it/s, loss=0.015, v_num=13]\nEpoch 19:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.013, v_num=13]\nEpoch 19:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.013, v_num=13]\nEpoch 19:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.013, v_num=13]\nEpoch 19:  92%|█████████▏| 48/52 [00:09<00:00,  5.31it/s, loss=0.013, v_num=13]\nEpoch 19:  98%|█████████▊| 51/52 [00:09<00:00,  5.54it/s, loss=0.013, v_num=13]\nEpoch 19: 100%|██████████| 52/52 [00:09<00:00,  5.61it/s, loss=0.013, v_num=13]\nEpoch 20:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.012, v_num=13]\nEpoch 20:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.012, v_num=13]\nEpoch 20:  87%|████████▋ | 45/52 [00:08<00:01,  5.01it/s, loss=0.012, v_num=13]\nEpoch 20:  92%|█████████▏| 48/52 [00:09<00:00,  5.28it/s, loss=0.012, v_num=13]\nEpoch 20:  98%|█████████▊| 51/52 [00:09<00:00,  5.52it/s, loss=0.012, v_num=13]\nEpoch 20: 100%|██████████| 52/52 [00:09<00:00,  5.59it/s, loss=0.012, v_num=13]\nEpoch 21:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.011, v_num=13]\nEpoch 21:  81%|████████  | 42/52 [00:08<00:02,  4.76it/s, loss=0.011, v_num=13]\nEpoch 21:  87%|████████▋ | 45/52 [00:08<00:01,  5.03it/s, loss=0.011, v_num=13]\nEpoch 21:  92%|█████████▏| 48/52 [00:09<00:00,  5.30it/s, loss=0.011, v_num=13]\nEpoch 21:  98%|█████████▊| 51/52 [00:09<00:00,  5.54it/s, loss=0.011, v_num=13]\nEpoch 21: 100%|██████████| 52/52 [00:09<00:00,  5.61it/s, loss=0.011, v_num=13]\nEpoch 22:  79%|███████▉  | 41/52 [00:08<00:02,  4.70it/s, loss=0.011, v_num=13]\nEpoch 22:  81%|████████  | 42/52 [00:08<00:02,  4.78it/s, loss=0.011, v_num=13]\nEpoch 22:  87%|████████▋ | 45/52 [00:08<00:01,  5.06it/s, loss=0.011, v_num=13]\nEpoch 22:  92%|█████████▏| 48/52 [00:09<00:00,  5.32it/s, loss=0.011, v_num=13]\nEpoch 22:  98%|█████████▊| 51/52 [00:09<00:00,  5.56it/s, loss=0.011, v_num=13]\nEpoch 22: 100%|██████████| 52/52 [00:09<00:00,  5.63it/s, loss=0.011, v_num=13]\nEpoch 23:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.030, v_num=13]\nEpoch 23:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.030, v_num=13]\nEpoch 23:  87%|████████▋ | 45/52 [00:08<00:01,  5.05it/s, loss=0.030, v_num=13]\nEpoch 23:  92%|█████████▏| 48/52 [00:09<00:00,  5.31it/s, loss=0.030, v_num=13]\nEpoch 23:  98%|█████████▊| 51/52 [00:09<00:00,  5.55it/s, loss=0.030, v_num=13]\nEpoch 23: 100%|██████████| 52/52 [00:09<00:00,  5.62it/s, loss=0.030, v_num=13]\nEpoch 24:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.226, v_num=13]\nEpoch 24:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.226, v_num=13]\nEpoch 24:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.226, v_num=13]\nEpoch 24:  92%|█████████▏| 48/52 [00:09<00:00,  5.31it/s, loss=0.226, v_num=13]\nEpoch 24: 100%|██████████| 52/52 [00:09<00:00,  5.62it/s, loss=0.226, v_num=13]\nEpoch 25:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.078, v_num=13]\nEpoch 25:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.078, v_num=13]\nEpoch 25:  87%|████████▋ | 45/52 [00:08<00:01,  5.03it/s, loss=0.078, v_num=13]\nEpoch 25:  92%|█████████▏| 48/52 [00:09<00:00,  5.29it/s, loss=0.078, v_num=13]\nEpoch 25: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.078, v_num=13]\nEpoch 26:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.048, v_num=13]\nEpoch 26:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.048, v_num=13]\nEpoch 26:  87%|████████▋ | 45/52 [00:08<00:01,  5.02it/s, loss=0.048, v_num=13]\nEpoch 26:  92%|█████████▏| 48/52 [00:09<00:00,  5.29it/s, loss=0.048, v_num=13]\nEpoch 26:  98%|█████████▊| 51/52 [00:09<00:00,  5.53it/s, loss=0.048, v_num=13]\nEpoch 26: 100%|██████████| 52/52 [00:09<00:00,  5.59it/s, loss=0.048, v_num=13]\nEpoch 27:  79%|███████▉  | 41/52 [00:08<00:02,  4.67it/s, loss=0.050, v_num=13]\nEpoch 27:  81%|████████  | 42/52 [00:08<00:02,  4.76it/s, loss=0.050, v_num=13]\nEpoch 27:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.050, v_num=13]\nEpoch 27:  92%|█████████▏| 48/52 [00:09<00:00,  5.30it/s, loss=0.050, v_num=13]\nEpoch 27: 100%|██████████| 52/52 [00:09<00:00,  5.61it/s, loss=0.050, v_num=13]\nEpoch 28:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.056, v_num=13]\nEpoch 28:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.056, v_num=13]\nEpoch 28:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.056, v_num=13]\nEpoch 28:  92%|█████████▏| 48/52 [00:09<00:00,  5.30it/s, loss=0.056, v_num=13]\nEpoch 28:  98%|█████████▊| 51/52 [00:09<00:00,  5.54it/s, loss=0.056, v_num=13]\nEpoch 28: 100%|██████████| 52/52 [00:09<00:00,  5.61it/s, loss=0.056, v_num=13]\nEpoch 29:  79%|███████▉  | 41/52 [00:08<00:02,  4.64it/s, loss=0.028, v_num=13]\nEpoch 29:  81%|████████  | 42/52 [00:08<00:02,  4.73it/s, loss=0.028, v_num=13]\nEpoch 29:  87%|████████▋ | 45/52 [00:08<00:01,  5.00it/s, loss=0.028, v_num=13]\nEpoch 29:  92%|█████████▏| 48/52 [00:09<00:00,  5.27it/s, loss=0.028, v_num=13]\nEpoch 29:  98%|█████████▊| 51/52 [00:09<00:00,  5.51it/s, loss=0.028, v_num=13]\nEpoch 29: 100%|██████████| 52/52 [00:09<00:00,  5.57it/s, loss=0.028, v_num=13]\nEpoch 30:  79%|███████▉  | 41/52 [00:08<00:02,  4.72it/s, loss=0.024, v_num=13]\nEpoch 30:  81%|████████  | 42/52 [00:08<00:02,  4.81it/s, loss=0.024, v_num=13]\nEpoch 30:  87%|████████▋ | 45/52 [00:08<00:01,  5.09it/s, loss=0.024, v_num=13]\nEpoch 30:  92%|█████████▏| 48/52 [00:08<00:00,  5.35it/s, loss=0.024, v_num=13]\nEpoch 30: 100%|██████████| 52/52 [00:09<00:00,  5.68it/s, loss=0.024, v_num=13]\nEpoch 31:  79%|███████▉  | 41/52 [00:08<00:02,  4.81it/s, loss=0.011, v_num=13]\nEpoch 31:  81%|████████  | 42/52 [00:08<00:02,  4.90it/s, loss=0.011, v_num=13]\nEpoch 31:  87%|████████▋ | 45/52 [00:08<00:01,  5.18it/s, loss=0.011, v_num=13]\nEpoch 31:  92%|█████████▏| 48/52 [00:08<00:00,  5.45it/s, loss=0.011, v_num=13]\nEpoch 31: 100%|██████████| 52/52 [00:09<00:00,  5.78it/s, loss=0.011, v_num=13]\nEpoch 32:  79%|███████▉  | 41/52 [00:08<00:02,  4.80it/s, loss=0.012, v_num=13]\nEpoch 32:  81%|████████  | 42/52 [00:08<00:02,  4.89it/s, loss=0.012, v_num=13]\nEpoch 32:  87%|████████▋ | 45/52 [00:08<00:01,  5.17it/s, loss=0.012, v_num=13]\nEpoch 32:  92%|█████████▏| 48/52 [00:08<00:00,  5.45it/s, loss=0.012, v_num=13]\nEpoch 32:  98%|█████████▊| 51/52 [00:08<00:00,  5.69it/s, loss=0.012, v_num=13]\nEpoch 32: 100%|██████████| 52/52 [00:09<00:00,  5.76it/s, loss=0.012, v_num=13]\nEpoch 33:  79%|███████▉  | 41/52 [00:08<00:02,  4.85it/s, loss=0.021, v_num=13]\nEpoch 33:  81%|████████  | 42/52 [00:08<00:02,  4.93it/s, loss=0.021, v_num=13]\nEpoch 33:  87%|████████▋ | 45/52 [00:08<00:01,  5.21it/s, loss=0.021, v_num=13]\nEpoch 33:  92%|█████████▏| 48/52 [00:08<00:00,  5.49it/s, loss=0.021, v_num=13]\nEpoch 33:  98%|█████████▊| 51/52 [00:08<00:00,  5.73it/s, loss=0.021, v_num=13]\nEpoch 33: 100%|██████████| 52/52 [00:08<00:00,  5.80it/s, loss=0.021, v_num=13]\nEpoch 34:  79%|███████▉  | 41/52 [00:08<00:02,  4.81it/s, loss=0.007, v_num=13]\nEpoch 34:  81%|████████  | 42/52 [00:08<00:02,  4.90it/s, loss=0.007, v_num=13]\nEpoch 34:  87%|████████▋ | 45/52 [00:08<00:01,  5.18it/s, loss=0.007, v_num=13]\nEpoch 34:  92%|█████████▏| 48/52 [00:08<00:00,  5.45it/s, loss=0.007, v_num=13]\nEpoch 34:  98%|█████████▊| 51/52 [00:08<00:00,  5.70it/s, loss=0.007, v_num=13]\nEpoch 34: 100%|██████████| 52/52 [00:09<00:00,  5.76it/s, loss=0.007, v_num=13]\nEpoch 35:  79%|███████▉  | 41/52 [00:08<00:02,  4.82it/s, loss=0.042, v_num=13]\nEpoch 35:  81%|████████  | 42/52 [00:08<00:02,  4.91it/s, loss=0.042, v_num=13]\nEpoch 35:  87%|████████▋ | 45/52 [00:08<00:01,  5.18it/s, loss=0.042, v_num=13]\nEpoch 35:  92%|█████████▏| 48/52 [00:08<00:00,  5.46it/s, loss=0.042, v_num=13]\nEpoch 35: 100%|██████████| 52/52 [00:08<00:00,  5.79it/s, loss=0.042, v_num=13]\nEpoch 36:  79%|███████▉  | 41/52 [00:08<00:02,  4.67it/s, loss=0.023, v_num=13]\nEpoch 36:  81%|████████  | 42/52 [00:08<00:02,  4.76it/s, loss=0.023, v_num=13]\nEpoch 36:  87%|████████▋ | 45/52 [00:08<00:01,  5.02it/s, loss=0.023, v_num=13]\nEpoch 36:  92%|█████████▏| 48/52 [00:09<00:00,  5.28it/s, loss=0.023, v_num=13]\nEpoch 36:  98%|█████████▊| 51/52 [00:09<00:00,  5.53it/s, loss=0.023, v_num=13]\nEpoch 36: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.023, v_num=13]\nEpoch 37:  79%|███████▉  | 41/52 [00:08<00:02,  4.67it/s, loss=0.020, v_num=13]\nEpoch 37:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.020, v_num=13]\nEpoch 37:  87%|████████▋ | 45/52 [00:08<00:01,  5.02it/s, loss=0.020, v_num=13]\nEpoch 37:  92%|█████████▏| 48/52 [00:09<00:00,  5.29it/s, loss=0.020, v_num=13]\nEpoch 37:  98%|█████████▊| 51/52 [00:09<00:00,  5.53it/s, loss=0.020, v_num=13]\nEpoch 37: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.020, v_num=13]\nEpoch 38:  79%|███████▉  | 41/52 [00:08<00:02,  4.69it/s, loss=0.005, v_num=13]\nEpoch 38:  81%|████████  | 42/52 [00:08<00:02,  4.78it/s, loss=0.005, v_num=13]\nEpoch 38:  87%|████████▋ | 45/52 [00:08<00:01,  5.06it/s, loss=0.005, v_num=13]\nEpoch 38:  92%|█████████▏| 48/52 [00:09<00:00,  5.32it/s, loss=0.005, v_num=13]\nEpoch 38:  98%|█████████▊| 51/52 [00:09<00:00,  5.56it/s, loss=0.005, v_num=13]\nEpoch 38: 100%|██████████| 52/52 [00:09<00:00,  5.63it/s, loss=0.005, v_num=13]\nEpoch 39:  79%|███████▉  | 41/52 [00:08<00:02,  4.67it/s, loss=0.004, v_num=13]\nEpoch 39:  81%|████████  | 42/52 [00:08<00:02,  4.76it/s, loss=0.004, v_num=13]\nEpoch 39:  87%|████████▋ | 45/52 [00:08<00:01,  5.02it/s, loss=0.004, v_num=13]\nEpoch 39:  92%|█████████▏| 48/52 [00:09<00:00,  5.29it/s, loss=0.004, v_num=13]\nEpoch 39:  98%|█████████▊| 51/52 [00:09<00:00,  5.53it/s, loss=0.004, v_num=13]\nEpoch 39: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.004, v_num=13]\nEpoch 40:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.005, v_num=13]\nEpoch 40:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.005, v_num=13]\nEpoch 40:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.005, v_num=13]\nEpoch 40:  92%|█████████▏| 48/52 [00:09<00:00,  5.31it/s, loss=0.005, v_num=13]\nEpoch 40:  98%|█████████▊| 51/52 [00:09<00:00,  5.55it/s, loss=0.005, v_num=13]\nEpoch 40: 100%|██████████| 52/52 [00:09<00:00,  5.62it/s, loss=0.005, v_num=13]\nEpoch 41:  79%|███████▉  | 41/52 [00:08<00:02,  4.67it/s, loss=0.003, v_num=13]\nEpoch 41:  81%|████████  | 42/52 [00:08<00:02,  4.76it/s, loss=0.003, v_num=13]\nEpoch 41:  87%|████████▋ | 45/52 [00:08<00:01,  5.03it/s, loss=0.003, v_num=13]\nEpoch 41:  92%|█████████▏| 48/52 [00:09<00:00,  5.30it/s, loss=0.003, v_num=13]\nEpoch 41:  98%|█████████▊| 51/52 [00:09<00:00,  5.53it/s, loss=0.003, v_num=13]\nEpoch 41: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.003, v_num=13]\nEpoch 42:  79%|███████▉  | 41/52 [00:08<00:02,  4.67it/s, loss=0.003, v_num=13]\nEpoch 42:  81%|████████  | 42/52 [00:08<00:02,  4.76it/s, loss=0.003, v_num=13]\nEpoch 42:  87%|████████▋ | 45/52 [00:08<00:01,  5.03it/s, loss=0.003, v_num=13]\nEpoch 42:  92%|█████████▏| 48/52 [00:09<00:00,  5.30it/s, loss=0.003, v_num=13]\nEpoch 42:  98%|█████████▊| 51/52 [00:09<00:00,  5.54it/s, loss=0.003, v_num=13]\nEpoch 42: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.003, v_num=13]\nEpoch 43:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.003, v_num=13]\nEpoch 43:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.003, v_num=13]\nEpoch 43:  87%|████████▋ | 45/52 [00:08<00:01,  5.01it/s, loss=0.003, v_num=13]\nEpoch 43:  92%|█████████▏| 48/52 [00:09<00:00,  5.28it/s, loss=0.003, v_num=13]\nEpoch 43:  98%|█████████▊| 51/52 [00:09<00:00,  5.52it/s, loss=0.003, v_num=13]\nEpoch 43: 100%|██████████| 52/52 [00:09<00:00,  5.59it/s, loss=0.003, v_num=13]\nEpoch 44:  79%|███████▉  | 41/52 [00:08<00:02,  4.69it/s, loss=0.003, v_num=13]\nEpoch 44:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.003, v_num=13]\nEpoch 44:  87%|████████▋ | 45/52 [00:08<00:01,  5.05it/s, loss=0.003, v_num=13]\nEpoch 44:  92%|█████████▏| 48/52 [00:09<00:00,  5.31it/s, loss=0.003, v_num=13]\nEpoch 44:  98%|█████████▊| 51/52 [00:09<00:00,  5.55it/s, loss=0.003, v_num=13]\nEpoch 44: 100%|██████████| 52/52 [00:09<00:00,  5.62it/s, loss=0.003, v_num=13]\nEpoch 45:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.003, v_num=13]\nEpoch 45:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.003, v_num=13]\nEpoch 45:  87%|████████▋ | 45/52 [00:08<00:01,  5.05it/s, loss=0.003, v_num=13]\nEpoch 45:  92%|█████████▏| 48/52 [00:09<00:00,  5.31it/s, loss=0.003, v_num=13]\nEpoch 45:  98%|█████████▊| 51/52 [00:09<00:00,  5.55it/s, loss=0.003, v_num=13]\nEpoch 45: 100%|██████████| 52/52 [00:09<00:00,  5.62it/s, loss=0.003, v_num=13]\nEpoch 46:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.024, v_num=13]\nEpoch 46:  81%|████████  | 42/52 [00:08<00:02,  4.74it/s, loss=0.024, v_num=13]\nEpoch 46:  87%|████████▋ | 45/52 [00:08<00:01,  5.02it/s, loss=0.024, v_num=13]\nEpoch 46:  92%|█████████▏| 48/52 [00:09<00:00,  5.27it/s, loss=0.024, v_num=13]\nEpoch 46: 100%|██████████| 52/52 [00:09<00:00,  5.59it/s, loss=0.024, v_num=13]\nEpoch 47:  79%|███████▉  | 41/52 [00:08<00:02,  4.64it/s, loss=0.107, v_num=13]\nEpoch 47:  81%|████████  | 42/52 [00:08<00:02,  4.73it/s, loss=0.107, v_num=13]\nEpoch 47:  87%|████████▋ | 45/52 [00:08<00:01,  5.00it/s, loss=0.107, v_num=13]\nEpoch 47:  92%|█████████▏| 48/52 [00:09<00:00,  5.27it/s, loss=0.107, v_num=13]\nEpoch 47:  98%|█████████▊| 51/52 [00:09<00:00,  5.50it/s, loss=0.107, v_num=13]\nEpoch 47: 100%|██████████| 52/52 [00:09<00:00,  5.57it/s, loss=0.107, v_num=13]\nEpoch 48:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.119, v_num=13]\nEpoch 48:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.119, v_num=13]\nEpoch 48:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.119, v_num=13]\nEpoch 48:  92%|█████████▏| 48/52 [00:09<00:00,  5.30it/s, loss=0.119, v_num=13]\nEpoch 48:  98%|█████████▊| 51/52 [00:09<00:00,  5.54it/s, loss=0.119, v_num=13]\nEpoch 48: 100%|██████████| 52/52 [00:09<00:00,  5.61it/s, loss=0.119, v_num=13]\nEpoch 49:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.045, v_num=13]\nEpoch 49:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.045, v_num=13]\nEpoch 49:  87%|████████▋ | 45/52 [00:08<00:01,  5.02it/s, loss=0.045, v_num=13]\nEpoch 49:  92%|█████████▏| 48/52 [00:09<00:00,  5.29it/s, loss=0.045, v_num=13]\nEpoch 49: 100%|██████████| 52/52 [00:09<00:00,  5.59it/s, loss=0.045, v_num=13]\nEpoch 50:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.044, v_num=13]\nEpoch 50:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.044, v_num=13]\nEpoch 50:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.044, v_num=13]\nEpoch 50:  92%|█████████▏| 48/52 [00:09<00:00,  5.30it/s, loss=0.044, v_num=13]\nEpoch 50: 100%|██████████| 52/52 [00:09<00:00,  5.62it/s, loss=0.044, v_num=13]\nEpoch 51:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.017, v_num=13]\nEpoch 51:  81%|████████  | 42/52 [00:08<00:02,  4.74it/s, loss=0.017, v_num=13]\nEpoch 51:  87%|████████▋ | 45/52 [00:08<00:01,  5.01it/s, loss=0.017, v_num=13]\nEpoch 51:  92%|█████████▏| 48/52 [00:09<00:00,  5.28it/s, loss=0.017, v_num=13]\nEpoch 51:  98%|█████████▊| 51/52 [00:09<00:00,  5.52it/s, loss=0.017, v_num=13]\nEpoch 51: 100%|██████████| 52/52 [00:09<00:00,  5.58it/s, loss=0.017, v_num=13]\nEpoch 52:  79%|███████▉  | 41/52 [00:08<00:02,  4.67it/s, loss=0.011, v_num=13]\nEpoch 52:  81%|████████  | 42/52 [00:08<00:02,  4.76it/s, loss=0.011, v_num=13]\nEpoch 52:  87%|████████▋ | 45/52 [00:08<00:01,  5.03it/s, loss=0.011, v_num=13]\nEpoch 52:  92%|█████████▏| 48/52 [00:09<00:00,  5.30it/s, loss=0.011, v_num=13]\nEpoch 52:  98%|█████████▊| 51/52 [00:09<00:00,  5.53it/s, loss=0.011, v_num=13]\nEpoch 52: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.011, v_num=13]\nEpoch 53:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.011, v_num=13]\nEpoch 53:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.011, v_num=13]\nEpoch 53:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.011, v_num=13]\nEpoch 53:  92%|█████████▏| 48/52 [00:09<00:00,  5.30it/s, loss=0.011, v_num=13]\nEpoch 53: 100%|██████████| 52/52 [00:09<00:00,  5.62it/s, loss=0.011, v_num=13]\nEpoch 54:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.014, v_num=13]\nEpoch 54:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.014, v_num=13]\nEpoch 54:  87%|████████▋ | 45/52 [00:08<00:01,  5.02it/s, loss=0.014, v_num=13]\nEpoch 54:  92%|█████████▏| 48/52 [00:09<00:00,  5.28it/s, loss=0.014, v_num=13]\nEpoch 54: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.014, v_num=13]\nEpoch 55:  79%|███████▉  | 41/52 [00:08<00:02,  4.69it/s, loss=0.004, v_num=13]\nEpoch 55:  81%|████████  | 42/52 [00:08<00:02,  4.78it/s, loss=0.004, v_num=13]\nEpoch 55:  87%|████████▋ | 45/52 [00:08<00:01,  5.05it/s, loss=0.004, v_num=13]\nEpoch 55:  92%|█████████▏| 48/52 [00:09<00:00,  5.32it/s, loss=0.004, v_num=13]\nEpoch 55: 100%|██████████| 52/52 [00:09<00:00,  5.63it/s, loss=0.004, v_num=13]\nEpoch 56:  79%|███████▉  | 41/52 [00:08<00:02,  4.71it/s, loss=0.008, v_num=13]\nEpoch 56:  81%|████████  | 42/52 [00:08<00:02,  4.81it/s, loss=0.008, v_num=13]\nEpoch 56:  87%|████████▋ | 45/52 [00:08<00:01,  5.08it/s, loss=0.008, v_num=13]\nEpoch 56:  92%|█████████▏| 48/52 [00:08<00:00,  5.35it/s, loss=0.008, v_num=13]\nEpoch 56:  98%|█████████▊| 51/52 [00:09<00:00,  5.58it/s, loss=0.008, v_num=13]\nEpoch 56: 100%|██████████| 52/52 [00:09<00:00,  5.65it/s, loss=0.008, v_num=13]\nEpoch 57:  79%|███████▉  | 41/52 [00:08<00:02,  4.67it/s, loss=0.042, v_num=13]\nEpoch 57:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.042, v_num=13]\nEpoch 57:  87%|████████▋ | 45/52 [00:08<00:01,  5.02it/s, loss=0.042, v_num=13]\nEpoch 57:  92%|█████████▏| 48/52 [00:09<00:00,  5.29it/s, loss=0.042, v_num=13]\nEpoch 57:  98%|█████████▊| 51/52 [00:09<00:00,  5.52it/s, loss=0.042, v_num=13]\nEpoch 57: 100%|██████████| 52/52 [00:09<00:00,  5.60it/s, loss=0.042, v_num=13]\nEpoch 58:  79%|███████▉  | 41/52 [00:08<00:02,  4.73it/s, loss=0.036, v_num=13]\nEpoch 58:  81%|████████  | 42/52 [00:08<00:02,  4.81it/s, loss=0.036, v_num=13]\nEpoch 58:  87%|████████▋ | 45/52 [00:08<00:01,  5.09it/s, loss=0.036, v_num=13]\nEpoch 58:  92%|█████████▏| 48/52 [00:08<00:00,  5.36it/s, loss=0.036, v_num=13]\nEpoch 58: 100%|██████████| 52/52 [00:09<00:00,  5.67it/s, loss=0.036, v_num=13]\nEpoch 59:  79%|███████▉  | 41/52 [00:08<00:02,  4.69it/s, loss=0.008, v_num=13]\nEpoch 59:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.008, v_num=13]\nEpoch 59:  87%|████████▋ | 45/52 [00:08<00:01,  5.04it/s, loss=0.008, v_num=13]\nEpoch 59:  92%|█████████▏| 48/52 [00:09<00:00,  5.31it/s, loss=0.008, v_num=13]\nEpoch 59:  98%|█████████▊| 51/52 [00:09<00:00,  5.55it/s, loss=0.008, v_num=13]\nEpoch 59: 100%|██████████| 52/52 [00:09<00:00,  5.63it/s, loss=0.008, v_num=13]\nEpoch 60:  79%|███████▉  | 41/52 [00:08<00:02,  4.75it/s, loss=0.013, v_num=13]\nEpoch 60:  81%|████████  | 42/52 [00:08<00:02,  4.84it/s, loss=0.013, v_num=13]\nEpoch 60:  87%|████████▋ | 45/52 [00:08<00:01,  5.12it/s, loss=0.013, v_num=13]\nEpoch 60:  92%|█████████▏| 48/52 [00:08<00:00,  5.38it/s, loss=0.013, v_num=13]\nEpoch 60:  98%|█████████▊| 51/52 [00:09<00:00,  5.63it/s, loss=0.013, v_num=13]\nEpoch 60: 100%|██████████| 52/52 [00:09<00:00,  5.69it/s, loss=0.013, v_num=13]\nEpoch 61:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.079, v_num=13]\nEpoch 61:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.079, v_num=13]\nEpoch 61:  87%|████████▋ | 45/52 [00:08<00:01,  5.03it/s, loss=0.079, v_num=13]\nEpoch 61:  92%|█████████▏| 48/52 [00:09<00:00,  5.29it/s, loss=0.079, v_num=13]\nValidating:  73%|███████▎  | 8/11 [00:00<00:00, 23.01it/s]\u001b[A\nEpoch 61: 100%|██████████| 52/52 [00:09<00:00,  5.59it/s, loss=0.079, v_num=13]\nEpoch 62:  79%|███████▉  | 41/52 [00:08<00:02,  4.66it/s, loss=0.016, v_num=13]\nEpoch 62:  81%|████████  | 42/52 [00:08<00:02,  4.75it/s, loss=0.016, v_num=13]\nEpoch 62:  87%|████████▋ | 45/52 [00:08<00:01,  5.02it/s, loss=0.016, v_num=13]\nEpoch 62:  92%|█████████▏| 48/52 [00:09<00:00,  5.28it/s, loss=0.016, v_num=13]\nEpoch 62:  98%|█████████▊| 51/52 [00:09<00:00,  5.52it/s, loss=0.016, v_num=13]\nEpoch 62: 100%|██████████| 52/52 [00:09<00:00,  5.58it/s, loss=0.016, v_num=13]\nEpoch 63:  79%|███████▉  | 41/52 [00:08<00:02,  4.68it/s, loss=0.018, v_num=13]\nEpoch 63:  81%|████████  | 42/52 [00:08<00:02,  4.77it/s, loss=0.018, v_num=13]\nEpoch 63:  87%|████████▋ | 45/52 [00:08<00:01,  5.03it/s, loss=0.018, v_num=13]\nEpoch 63:  92%|█████████▏| 48/52 [00:09<00:00,  5.30it/s, loss=0.018, v_num=13]\nEpoch 63:  98%|█████████▊| 51/52 [00:09<00:00,  5.54it/s, loss=0.018, v_num=13]\nEpoch 63: 100%|██████████| 52/52 [00:09<00:00,  5.61it/s, loss=0.018, v_num=13]\nEpoch 64:   0%|          | 0/52 [00:00<?, ?it/s, loss=0.018, v_num=13]Saving latest checkpoint..\nEpoch 64:   0%|          | 0/52 [00:00<?, ?it/s, loss=0.018, v_num=13]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1)\n",
    "trainer.fit(roberta, corefx_dm)"
   ]
  }
 ]
}