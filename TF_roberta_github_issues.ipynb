{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit ('base': conda)",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports and global variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from transformers import RobertaTokenizerFast, TFRobertaModel, TFRobertaForSequenceClassification, RobertaForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(os.getcwd(), \"Data\")\n",
    "MODELS_FOLDER = os.path.join(os.getcwd(), \"Models\")\n",
    "TF_LOGS_FOLDER = os.path.join(os.getcwd(), \"tf_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_FOLDER, \"corefx_cleaned_train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(DATA_FOLDER, \"corefx_cleaned_val.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LEN = len(train_df)\n",
    "VAL_LEN = len(val_df)\n",
    "del train_df\n",
    "del val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 128\n",
    "DROPOUT = 0.3\n",
    "SEED = 2020"
   ]
  },
  {
   "source": [
    "# Create Tensorflow datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.experimental.CsvDataset(os.path.join(DATA_FOLDER, \"corefx_cleaned_train.csv\"), [tf.string, tf.int32], header=True)\n",
    "val_dataset = tf.data.experimental.CsvDataset(os.path.join(DATA_FOLDER, \"corefx_cleaned_val.csv\"), [tf.string, tf.int32], header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "source": [
    "## Define transformations to do on datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_py_function(sentences):\n",
    "    decoded = []\n",
    "    for sentence in sentences.numpy():\n",
    "        decoded.append(sentence.decode())\n",
    "    encoded = tokenizer(decoded, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='tf')\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "    attention_mask = encoded[\"attention_mask\"]\n",
    "    return (input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to use tf.py_function because tokenizer doesn't operate on tensors\n",
    "def encode_data(batch_x, batch_y):\n",
    "    input_ids, attention_mask = tf.py_function(tf_py_function, [batch_x], (tf.int32, tf.int32))\n",
    "    input_ids.set_shape([None, MAX_LEN])\n",
    "    attention_mask.set_shape([None, MAX_LEN])\n",
    "    return (input_ids, attention_mask, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_dict(input_ids, attention_mask, labels):\n",
    "    return ({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels)"
   ]
  },
  {
   "source": [
    "## Apply transformations and prepare datasets for training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(TRAIN_LEN, seed=SEED, reshuffle_each_iteration=True).batch(BATCH_SIZE).map(encode_data, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(map_to_dict, num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat().prefetch(1)\n",
    "val_dataset = val_dataset.shuffle(VAL_LEN, seed=SEED, reshuffle_each_iteration=True).batch(BATCH_SIZE).map(encode_data, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(map_to_dict, num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat().prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, 'lookup.json')) as json_file: \n",
    "    lookup = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaForSequenceClassification: [&#39;lm_head&#39;]\n- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: [&#39;classifier&#39;]\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "github_model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(lookup.keys()) // 2, hidden_dropout_prob=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(epoch, lr):\n",
    "  if epoch < 10 or epoch > 40:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "\n",
    "github_model.compile(optimizer=opt, loss=loss, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "reduceLr = ReduceLROnPlateau(monitor='val_loss', mode='min', verbose=1, patience=5, factor=0.75, min_lr=1e-6)\n",
    "checkpointer = ModelCheckpoint(filepath= os.path.join( MODELS_FOLDER, \"github_model_weights.hdf5\"), verbose=1, save_best_only=True)\n",
    "scheduler = LearningRateScheduler(schedule)\n",
    "# use tensorboard\n",
    "log_dir = os.path.join(TF_LOGS_FOLDER, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/100\nWARNING:tensorflow:Gradients do not exist for variables [&#39;tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0&#39;, &#39;tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0&#39;] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables [&#39;tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0&#39;, &#39;tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0&#39;] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables [&#39;tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0&#39;, &#39;tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0&#39;] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables [&#39;tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0&#39;, &#39;tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0&#39;] when minimizing the loss.\n 1/40 [..............................] - ETA: 0s - loss: 3.1422 - accuracy: 0.0000e+00WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\nInstructions for updating:\nuse `tf.profiler.experimental.stop` instead.\n40/40 [==============================] - ETA: 0s - loss: 2.6597 - accuracy: 0.2711\nEpoch 00001: val_loss improved from inf to 2.47690, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 21s 527ms/step - loss: 2.6597 - accuracy: 0.2711 - val_loss: 2.4769 - val_accuracy: 0.2937\nEpoch 2/100\n40/40 [==============================] - ETA: 0s - loss: 2.3354 - accuracy: 0.3175\nEpoch 00002: val_loss improved from 2.47690 to 2.14337, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 19s 485ms/step - loss: 2.3354 - accuracy: 0.3175 - val_loss: 2.1434 - val_accuracy: 0.4437\nEpoch 3/100\n40/40 [==============================] - ETA: 0s - loss: 2.1112 - accuracy: 0.3722\nEpoch 00003: val_loss improved from 2.14337 to 1.85898, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 19s 463ms/step - loss: 2.1112 - accuracy: 0.3722 - val_loss: 1.8590 - val_accuracy: 0.4719\nEpoch 4/100\n40/40 [==============================] - ETA: 0s - loss: 1.8329 - accuracy: 0.4794\nEpoch 00004: val_loss improved from 1.85898 to 1.62820, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 19s 477ms/step - loss: 1.8329 - accuracy: 0.4794 - val_loss: 1.6282 - val_accuracy: 0.5781\nEpoch 5/100\n40/40 [==============================] - ETA: 0s - loss: 1.5751 - accuracy: 0.5786\nEpoch 00005: val_loss improved from 1.62820 to 1.43296, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 19s 467ms/step - loss: 1.5751 - accuracy: 0.5786 - val_loss: 1.4330 - val_accuracy: 0.6125\nEpoch 6/100\n40/40 [==============================] - ETA: 0s - loss: 1.3681 - accuracy: 0.6452\nEpoch 00006: val_loss improved from 1.43296 to 1.40135, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 19s 475ms/step - loss: 1.3681 - accuracy: 0.6452 - val_loss: 1.4014 - val_accuracy: 0.6406\nEpoch 7/100\n40/40 [==============================] - ETA: 0s - loss: 1.1413 - accuracy: 0.7222\nEpoch 00007: val_loss improved from 1.40135 to 1.32851, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 19s 464ms/step - loss: 1.1413 - accuracy: 0.7222 - val_loss: 1.3285 - val_accuracy: 0.6594\nEpoch 8/100\n40/40 [==============================] - ETA: 0s - loss: 1.0828 - accuracy: 0.7278\nEpoch 00008: val_loss improved from 1.32851 to 1.23520, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 20s 500ms/step - loss: 1.0828 - accuracy: 0.7278 - val_loss: 1.2352 - val_accuracy: 0.6875\nEpoch 9/100\n40/40 [==============================] - ETA: 0s - loss: 0.8745 - accuracy: 0.7643\nEpoch 00009: val_loss improved from 1.23520 to 1.15523, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 19s 467ms/step - loss: 0.8745 - accuracy: 0.7643 - val_loss: 1.1552 - val_accuracy: 0.7031\nEpoch 10/100\n40/40 [==============================] - ETA: 0s - loss: 0.7663 - accuracy: 0.8000\nEpoch 00010: val_loss improved from 1.15523 to 1.10089, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 20s 495ms/step - loss: 0.7663 - accuracy: 0.8000 - val_loss: 1.1009 - val_accuracy: 0.7312\nEpoch 11/100\n40/40 [==============================] - ETA: 0s - loss: 0.7031 - accuracy: 0.8214\nEpoch 00011: val_loss improved from 1.10089 to 1.04435, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 19s 469ms/step - loss: 0.7031 - accuracy: 0.8214 - val_loss: 1.0443 - val_accuracy: 0.7531\nEpoch 12/100\n40/40 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.8627\nEpoch 00012: val_loss did not improve from 1.04435\n40/40 [==============================] - 17s 432ms/step - loss: 0.5493 - accuracy: 0.8627 - val_loss: 1.0595 - val_accuracy: 0.7500\nEpoch 13/100\n40/40 [==============================] - ETA: 0s - loss: 0.5217 - accuracy: 0.8722\nEpoch 00013: val_loss did not improve from 1.04435\n40/40 [==============================] - 17s 432ms/step - loss: 0.5217 - accuracy: 0.8722 - val_loss: 1.0852 - val_accuracy: 0.7531\nEpoch 14/100\n40/40 [==============================] - ETA: 0s - loss: 0.4363 - accuracy: 0.8929\nEpoch 00014: val_loss improved from 1.04435 to 0.99762, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 19s 482ms/step - loss: 0.4363 - accuracy: 0.8929 - val_loss: 0.9976 - val_accuracy: 0.7844\nEpoch 15/100\n40/40 [==============================] - ETA: 0s - loss: 0.4448 - accuracy: 0.8968\nEpoch 00015: val_loss did not improve from 0.99762\n40/40 [==============================] - 17s 437ms/step - loss: 0.4448 - accuracy: 0.8968 - val_loss: 1.0455 - val_accuracy: 0.7812\nEpoch 16/100\n40/40 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.9119\nEpoch 00016: val_loss did not improve from 0.99762\n40/40 [==============================] - 17s 432ms/step - loss: 0.3400 - accuracy: 0.9119 - val_loss: 1.0098 - val_accuracy: 0.7750\nEpoch 17/100\n40/40 [==============================] - ETA: 0s - loss: 0.3013 - accuracy: 0.9325\nEpoch 00017: val_loss did not improve from 0.99762\n40/40 [==============================] - 18s 440ms/step - loss: 0.3013 - accuracy: 0.9325 - val_loss: 1.0125 - val_accuracy: 0.7750\nEpoch 18/100\n40/40 [==============================] - ETA: 0s - loss: 0.3048 - accuracy: 0.9302\nEpoch 00018: val_loss did not improve from 0.99762\n40/40 [==============================] - 17s 435ms/step - loss: 0.3048 - accuracy: 0.9302 - val_loss: 1.0659 - val_accuracy: 0.7719\nEpoch 19/100\n40/40 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.9476\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 1.5246353086695308e-05.\n\nEpoch 00019: val_loss did not improve from 0.99762\n40/40 [==============================] - 18s 443ms/step - loss: 0.2429 - accuracy: 0.9476 - val_loss: 1.0530 - val_accuracy: 0.7844\nEpoch 20/100\n40/40 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9603\nEpoch 00020: val_loss did not improve from 0.99762\n40/40 [==============================] - 17s 435ms/step - loss: 0.2024 - accuracy: 0.9603 - val_loss: 1.0171 - val_accuracy: 0.7875\nEpoch 21/100\n40/40 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9571\nEpoch 00021: val_loss improved from 0.99762 to 0.98939, saving model to d:\\work\\RobertaGithubIssuesClassification\\Models\\github_model_weights.hdf5\n40/40 [==============================] - 19s 469ms/step - loss: 0.1937 - accuracy: 0.9571 - val_loss: 0.9894 - val_accuracy: 0.7969\nEpoch 22/100\n40/40 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.9635\nEpoch 00022: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 433ms/step - loss: 0.1850 - accuracy: 0.9635 - val_loss: 1.0127 - val_accuracy: 0.7937\nEpoch 23/100\n40/40 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9651\nEpoch 00023: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 433ms/step - loss: 0.1610 - accuracy: 0.9651 - val_loss: 1.0434 - val_accuracy: 0.7875\nEpoch 24/100\n40/40 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9651\nEpoch 00024: val_loss did not improve from 0.98939\n40/40 [==============================] - 18s 441ms/step - loss: 0.1770 - accuracy: 0.9651 - val_loss: 1.0515 - val_accuracy: 0.7781\nEpoch 25/100\n40/40 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9746\nEpoch 00025: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 432ms/step - loss: 0.1363 - accuracy: 0.9746 - val_loss: 1.0240 - val_accuracy: 0.7937\nEpoch 26/100\n40/40 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9738\nEpoch 00026: ReduceLROnPlateau reducing learning rate to 5.678333877767727e-06.\n\nEpoch 00026: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 433ms/step - loss: 0.1474 - accuracy: 0.9738 - val_loss: 1.0873 - val_accuracy: 0.7875\nEpoch 27/100\n40/40 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9714\nEpoch 00027: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 431ms/step - loss: 0.1575 - accuracy: 0.9714 - val_loss: 1.0447 - val_accuracy: 0.7969\nEpoch 28/100\n40/40 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9778\nEpoch 00028: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 431ms/step - loss: 0.1256 - accuracy: 0.9778 - val_loss: 1.0164 - val_accuracy: 0.8000\nEpoch 29/100\n40/40 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.9667\nEpoch 00029: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 431ms/step - loss: 0.1428 - accuracy: 0.9667 - val_loss: 1.0678 - val_accuracy: 0.7906\nEpoch 30/100\n40/40 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9794\nEpoch 00030: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 432ms/step - loss: 0.1210 - accuracy: 0.9794 - val_loss: 1.0196 - val_accuracy: 0.8031\nEpoch 31/100\n40/40 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9770\nEpoch 00031: ReduceLROnPlateau reducing learning rate to 2.583061871064274e-06.\n\nEpoch 00031: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 431ms/step - loss: 0.1209 - accuracy: 0.9770 - val_loss: 1.0400 - val_accuracy: 0.8031\nEpoch 32/100\n40/40 [==============================] - ETA: 0s - loss: 0.1288 - accuracy: 0.9770\nEpoch 00032: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 432ms/step - loss: 0.1288 - accuracy: 0.9770 - val_loss: 1.0490 - val_accuracy: 0.8031\nEpoch 33/100\n40/40 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9770\nEpoch 00033: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 431ms/step - loss: 0.1176 - accuracy: 0.9770 - val_loss: 1.0467 - val_accuracy: 0.8031\nEpoch 34/100\n40/40 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9849\nEpoch 00034: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 431ms/step - loss: 0.1115 - accuracy: 0.9849 - val_loss: 1.0215 - val_accuracy: 0.8000\nEpoch 35/100\n40/40 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9833\nEpoch 00035: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 432ms/step - loss: 0.1131 - accuracy: 0.9833 - val_loss: 1.0702 - val_accuracy: 0.7906\nEpoch 36/100\n40/40 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9778\nEpoch 00036: ReduceLROnPlateau reducing learning rate to 1.1750292685519526e-06.\n\nEpoch 00036: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 433ms/step - loss: 0.1170 - accuracy: 0.9778 - val_loss: 1.0624 - val_accuracy: 0.7969\nEpoch 37/100\n40/40 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9841\nEpoch 00037: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 432ms/step - loss: 0.1101 - accuracy: 0.9841 - val_loss: 1.0429 - val_accuracy: 0.7969\nEpoch 38/100\n40/40 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9810\nEpoch 00038: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 435ms/step - loss: 0.1148 - accuracy: 0.9810 - val_loss: 1.0534 - val_accuracy: 0.7969\nEpoch 39/100\n40/40 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9786\nEpoch 00039: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 431ms/step - loss: 0.1296 - accuracy: 0.9786 - val_loss: 1.0374 - val_accuracy: 0.7969\nEpoch 40/100\n40/40 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9810\nEpoch 00040: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 431ms/step - loss: 0.1110 - accuracy: 0.9810 - val_loss: 1.0329 - val_accuracy: 0.7969\nEpoch 41/100\n40/40 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9754\nEpoch 00041: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 431ms/step - loss: 0.1193 - accuracy: 0.9754 - val_loss: 1.0364 - val_accuracy: 0.8031\nEpoch 42/100\n40/40 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9812\nEpoch 00042: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 437ms/step - loss: 0.1082 - accuracy: 0.9812 - val_loss: 1.0445 - val_accuracy: 0.8031\nEpoch 43/100\n40/40 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9746\nEpoch 00043: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 435ms/step - loss: 0.1200 - accuracy: 0.9746 - val_loss: 1.0576 - val_accuracy: 0.8000\nEpoch 44/100\n40/40 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9841\nEpoch 00044: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 435ms/step - loss: 0.1129 - accuracy: 0.9841 - val_loss: 1.0426 - val_accuracy: 0.8031\nEpoch 45/100\n40/40 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9802\nEpoch 00045: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 430ms/step - loss: 0.1076 - accuracy: 0.9802 - val_loss: 1.0615 - val_accuracy: 0.7969\nEpoch 46/100\n40/40 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.9817\nEpoch 00046: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 433ms/step - loss: 0.1097 - accuracy: 0.9817 - val_loss: 1.0696 - val_accuracy: 0.8000\nEpoch 47/100\n40/40 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9849\nEpoch 00047: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 434ms/step - loss: 0.1017 - accuracy: 0.9849 - val_loss: 1.0627 - val_accuracy: 0.8000\nEpoch 48/100\n40/40 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9873\nEpoch 00048: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 432ms/step - loss: 0.1004 - accuracy: 0.9873 - val_loss: 1.0580 - val_accuracy: 0.8031\nEpoch 49/100\n40/40 [==============================] - ETA: 0s - loss: 0.1137 - accuracy: 0.9825\nEpoch 00049: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 432ms/step - loss: 0.1137 - accuracy: 0.9825 - val_loss: 1.0682 - val_accuracy: 0.7969\nEpoch 50/100\n40/40 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9865\nEpoch 00050: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 431ms/step - loss: 0.1018 - accuracy: 0.9865 - val_loss: 1.0415 - val_accuracy: 0.8000\nEpoch 51/100\n40/40 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.9825\nEpoch 00051: val_loss did not improve from 0.98939\n40/40 [==============================] - 17s 430ms/step - loss: 0.1088 - accuracy: 0.9825 - val_loss: 1.0665 - val_accuracy: 0.8000\nEpoch 00051: early stopping\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tensorflow.python.keras.callbacks.History at 0x1ad16811088&gt;"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "github_model.fit(train_dataset, validation_data=val_dataset, steps_per_epoch= TRAIN_LEN // BATCH_SIZE, epochs=100, validation_steps= VAL_LEN // BATCH_SIZE, callbacks=[earlystopping, reduceLr, checkpointer, tensorboard, scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save TF model (Huggingface format and SavedModel format) and create Pytorch model from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(&#39;d:\\\\work\\\\RobertaGithubIssuesClassification\\\\Models\\\\tf\\\\vocab.json&#39;,\n &#39;d:\\\\work\\\\RobertaGithubIssuesClassification\\\\Models\\\\tf\\\\merges.txt&#39;,\n &#39;d:\\\\work\\\\RobertaGithubIssuesClassification\\\\Models\\\\tf\\\\special_tokens_map.json&#39;,\n &#39;d:\\\\work\\\\RobertaGithubIssuesClassification\\\\Models\\\\tf\\\\added_tokens.json&#39;)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "github_model.load_weights(os.path.join( MODELS_FOLDER, \"github_model_weights.hdf5\"))\n",
    "github_model.save_pretrained(os.path.join(MODELS_FOLDER, \"tf\"))\n",
    "tokenizer.save_pretrained(os.path.join(MODELS_FOLDER, \"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n\nSome weights of RobertaForSequenceClassification were not initialized from the TF 2.0 model and are newly initialized: [&#39;roberta.embeddings.position_ids&#39;]\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "pytorch_model = RobertaForSequenceClassification.from_pretrained(os.path.join(MODELS_FOLDER, \"tf\"), from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(&#39;d:\\\\work\\\\RobertaGithubIssuesClassification\\\\Models\\\\pt\\\\vocab.json&#39;,\n &#39;d:\\\\work\\\\RobertaGithubIssuesClassification\\\\Models\\\\pt\\\\merges.txt&#39;,\n &#39;d:\\\\work\\\\RobertaGithubIssuesClassification\\\\Models\\\\pt\\\\special_tokens_map.json&#39;,\n &#39;d:\\\\work\\\\RobertaGithubIssuesClassification\\\\Models\\\\pt\\\\added_tokens.json&#39;)"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "pytorch_model.save_pretrained(os.path.join(MODELS_FOLDER, \"pt\"))\n",
    "tokenizer.save_pretrained(os.path.join(MODELS_FOLDER, \"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callable = tf.function(github_model.call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_function = callable.get_concrete_function([tf.TensorSpec([None, MAX_LEN], tf.int32, name=\"input_ids\"), tf.TensorSpec([None, MAX_LEN], tf.int32, name=\"attention_mask\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nWARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nINFO:tensorflow:Assets written to: d:\\work\\RobertaGithubIssuesClassification\\Models\\Serving\\1\\assets\n"
    }
   ],
   "source": [
    "tf.saved_model.save(github_model, os.path.join(MODELS_FOLDER, \"Serving\", \"1\"), signatures=concrete_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try models on test issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = \"AppDomain.SetPrincipalPolicy(PrincipalPolicy.WindowsPrincipal) works only once. Setting the PrincipalPolicy on the current AppDomain to WindowsPrincipal works only for the first thread being started. Any subsequent thread has Thread.CurrentPrincipal evaluated to NULL.\"\n",
    "label = \"area-System.Security\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tf = tokenizer(issue, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='tf', add_special_tokens=True, return_token_type_ids=False)\n",
    "encoded_pt = tokenizer(issue, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt', add_special_tokens=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "All model checkpoint weights were used when initializing TFRobertaForSequenceClassification.\n\nAll the weights of TFRobertaForSequenceClassification were initialized from the model checkpoint at d:\\work\\RobertaGithubIssuesClassification\\Models\\tf.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
    }
   ],
   "source": [
    "github_model = TFRobertaForSequenceClassification.from_pretrained(os.path.join(MODELS_FOLDER, \"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(array([[-0.03901269, -0.70156264, -0.00813974,  5.8706336 ,  0.60016686,\n        -0.90594006,  1.7966572 , -1.9033011 , -0.07496928, -0.7290467 ,\n        -2.3262384 ,  1.2474042 , -0.74498063, -0.64792466, -0.71970916,\n         2.2690477 , -0.9807402 , -1.0907569 , -1.7531682 ,  1.3895686 ,\n        -0.98030645, -0.19943248]], dtype=float32),)\n"
    }
   ],
   "source": [
    "tf_result = github_model.predict([encoded_tf['input_ids'], encoded_tf['attention_mask']])\n",
    "print(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(tensor([[-0.0390, -0.7016, -0.0081,  5.8706,  0.6002, -0.9059,  1.7967, -1.9033,\n         -0.0750, -0.7290, -2.3262,  1.2474, -0.7450, -0.6479, -0.7197,  2.2690,\n         -0.9807, -1.0908, -1.7532,  1.3896, -0.9803, -0.1994]],\n       grad_fn=&lt;AddmmBackward&gt;),)\n"
    }
   ],
   "source": [
    "pytorch_model.eval()\n",
    "pt_result = pytorch_model(input_ids=encoded_pt[\"input_ids\"], attention_mask=encoded_pt[\"attention_mask\"])\n",
    "print(pt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "np.allclose(tf_result[0], pt_result[0].detach().numpy(), rtol=1e-03, atol=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&#39;area-System.Security&#39;"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "lookup[str(np.argmax(tf_result))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to ONNX and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "pytorch_model.eval()\n",
    "torch.onnx.export(pytorch_model,               # model being run\n",
    "                  (encoded_pt[\"input_ids\"], encoded_pt[\"attention_mask\"]),  # model input (or a tuple for multiple inputs)\n",
    "                  os.path.join(MODELS_FOLDER, \"roberta_github_issues.onnx\"),   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=12,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input_ids', 'attention_mask'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input_ids' : {0 : 'batch_size'},\n",
    "                                'attention_mask' : {0 : 'batch_size'},\n",
    "                                'output' : {0 : 'batch_size'}}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(os.path.join(MODELS_FOLDER, \"roberta_github_issues.onnx\"))\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(os.path.join(MODELS_FOLDER, \"roberta_github_issues.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&#39;attention_mask&#39;"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "ort_session.get_inputs()[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(encoded_pt[\"input_ids\"]), ort_session.get_inputs()[1].name: to_numpy(encoded_pt[\"attention_mask\"])}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[array([[-0.03901183, -0.70156324, -0.00814154,  5.8706365 ,  0.6001704 ,\n        -0.9059382 ,  1.7966552 , -1.9032998 , -0.07497119, -0.72904754,\n        -2.326238  ,  1.2474039 , -0.744979  , -0.64792377, -0.7197121 ,\n         2.2690408 , -0.9807419 , -1.090755  , -1.7531687 ,  1.3895706 ,\n        -0.9803071 , -0.19943394]], dtype=float32)]\n(tensor([[-0.0390, -0.7016, -0.0081,  5.8706,  0.6002, -0.9059,  1.7967, -1.9033,\n         -0.0750, -0.7290, -2.3262,  1.2474, -0.7450, -0.6479, -0.7197,  2.2690,\n         -0.9807, -1.0908, -1.7532,  1.3896, -0.9803, -0.1994]],\n       grad_fn=&lt;AddmmBackward&gt;),)\n(array([[-0.03901269, -0.70156264, -0.00813974,  5.8706336 ,  0.60016686,\n        -0.90594006,  1.7966572 , -1.9033011 , -0.07496928, -0.7290467 ,\n        -2.3262384 ,  1.2474042 , -0.74498063, -0.64792466, -0.71970916,\n         2.2690477 , -0.9807402 , -1.0907569 , -1.7531682 ,  1.3895686 ,\n        -0.98030645, -0.19943248]], dtype=float32),)\n"
    }
   ],
   "source": [
    "print(ort_outs)\n",
    "print(pt_result)\n",
    "print(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "np.allclose(ort_outs, tf_result[0], rtol=1e-03, atol=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "np.allclose(ort_outs, pt_result[0].detach().numpy(), rtol=1e-03, atol=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "index : 3, category : area-System.Security\n"
    }
   ],
   "source": [
    "index = np.argmax(to_numpy(pt_result[0]))\n",
    "print(f\"index : {index}, category : {lookup[str(index)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.functional.softmax(pt_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "area-System.Net             : 0.25% confidence\narea-Infrastructure         : 0.13% confidence\narea-System.ComponentModel  : 0.26% confidence\narea-System.Security        : 91.46% confidence\narea-System.Runtime         : 0.47% confidence\narea-System.IO              : 0.10% confidence\narea-System.Xml             : 1.56% confidence\narea-System.Collections     : 0.04% confidence\narea-System.Threading       : 0.24% confidence\narea-System.Reflection      : 0.12% confidence\narea-System.Memory          : 0.03% confidence\narea-System.Diagnostics     : 0.90% confidence\narea-Serialization          : 0.12% confidence\narea-System.Drawing         : 0.13% confidence\narea-Meta                   : 0.13% confidence\narea-System.Data            : 2.50% confidence\narea-Microsoft.CSharp       : 0.10% confidence\narea-System.Numerics        : 0.09% confidence\narea-System.Text            : 0.04% confidence\narea-System.Globalization   : 1.04% confidence\narea-System.Linq            : 0.10% confidence\narea-System.Console         : 0.21% confidence\n"
    }
   ],
   "source": [
    "for i, value in enumerate(softmax[0]):\n",
    "    print(f\"{lookup[str(i)] : <27} : {value.item() * 100 :.2f}% confidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}