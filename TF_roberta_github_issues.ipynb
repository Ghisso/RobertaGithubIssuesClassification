{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit ('base': conda)",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports and global variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizerFast, TFRobertaModel, TFRobertaForSequenceClassification, RobertaForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(\"./\", \"Data\")\n",
    "MODELS_PATH = os.path.join(\"./\", \"Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_FOLDER, \"corefx_cleaned_train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(DATA_FOLDER, \"corefx_cleaned_val.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LEN = len(train_df)\n",
    "VAL_LEN = len(val_df)\n",
    "del train_df\n",
    "del val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 64"
   ]
  },
  {
   "source": [
    "# Create Tensorflow datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.experimental.CsvDataset(os.path.join(DATA_FOLDER, \"corefx_cleaned_train.csv\"), [tf.string, tf.int32], header=True)\n",
    "val_dataset = tf.data.experimental.CsvDataset(os.path.join(DATA_FOLDER, \"corefx_cleaned_val.csv\"), [tf.string, tf.int32], header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "source": [
    "## Define transformations to do on datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_py_function(sentences):\n",
    "    decoded = []\n",
    "    for sentence in sentences.numpy():\n",
    "        decoded.append(sentence.decode())\n",
    "    encoded = tokenizer(decoded, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='tf')\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "    attention_mask = encoded[\"attention_mask\"]\n",
    "    return (input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to use tf.py_function because tokenizer doesn't operate on tensors\n",
    "def encode_data(batch_x, batch_y):\n",
    "    input_ids, attention_mask = tf.py_function(tf_py_function, [batch_x], (tf.int32, tf.int32))\n",
    "    input_ids.set_shape([None, MAX_LEN])\n",
    "    attention_mask.set_shape([None, MAX_LEN])\n",
    "    return (input_ids, attention_mask, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_dict(input_ids, attention_mask, labels):\n",
    "    return ({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels)"
   ]
  },
  {
   "source": [
    "## Apply transformations and prepare datasets for training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(encode_data, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(map_to_dict, num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat().prefetch(1)\n",
    "val_dataset = val_dataset.map(encode_data, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(map_to_dict, num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat().prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, 'lookup.json')) as json_file: \n",
    "    lookup = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "github_model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(lookup.keys()) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-4, epsilon=1e-08)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "\n",
    "github_model.compile(optimizer=opt, loss=loss, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\nWARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n41/41 [==============================] - 11s 274ms/step - loss: 2.4006 - accuracy: 0.3135 - val_loss: 2.3642 - val_accuracy: 0.3777\nEpoch 2/10\n41/41 [==============================] - 10s 233ms/step - loss: 1.9358 - accuracy: 0.4574 - val_loss: 1.5855 - val_accuracy: 0.5913\nEpoch 3/10\n41/41 [==============================] - 10s 234ms/step - loss: 1.4165 - accuracy: 0.6324 - val_loss: 1.2952 - val_accuracy: 0.6718\nEpoch 4/10\n41/41 [==============================] - 10s 234ms/step - loss: 1.0548 - accuracy: 0.7376 - val_loss: 1.2349 - val_accuracy: 0.6935\nEpoch 5/10\n41/41 [==============================] - 10s 235ms/step - loss: 0.8216 - accuracy: 0.7926 - val_loss: 1.0976 - val_accuracy: 0.7368\nEpoch 6/10\n41/41 [==============================] - 10s 234ms/step - loss: 0.6164 - accuracy: 0.8491 - val_loss: 1.2145 - val_accuracy: 0.7183\nEpoch 7/10\n41/41 [==============================] - 10s 235ms/step - loss: 0.5167 - accuracy: 0.8700 - val_loss: 1.0910 - val_accuracy: 0.7616\nEpoch 8/10\n41/41 [==============================] - 10s 235ms/step - loss: 0.3835 - accuracy: 0.9056 - val_loss: 1.1148 - val_accuracy: 0.7523\nEpoch 9/10\n41/41 [==============================] - 10s 235ms/step - loss: 0.2995 - accuracy: 0.9265 - val_loss: 1.0819 - val_accuracy: 0.7833\nEpoch 10/10\n41/41 [==============================] - 10s 236ms/step - loss: 0.2961 - accuracy: 0.9272 - val_loss: 1.2381 - val_accuracy: 0.7616\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1f5aaaaea08>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "github_model.fit(train_dataset, validation_data=val_dataset, steps_per_epoch= TRAIN_LEN // BATCH_SIZE + 1  , epochs=10, validation_steps= VAL_LEN // BATCH_SIZE +1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save TF model (Huggingface format and SavedModel format) and create Pytorch model from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('./Models\\\\tf\\\\vocab.json',\n './Models\\\\tf\\\\merges.txt',\n './Models\\\\tf\\\\special_tokens_map.json',\n './Models\\\\tf\\\\added_tokens.json')"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "github_model.save_pretrained(os.path.join(MODELS_PATH, \"tf\"))\n",
    "tokenizer.save_pretrained(os.path.join(MODELS_PATH, \"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n\nAll the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\nIf your task is similar to the task the model of the ckeckpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
    }
   ],
   "source": [
    "pytorch_model = RobertaForSequenceClassification.from_pretrained(os.path.join(MODELS_PATH, \"tf\"), from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('./Models\\\\pt\\\\vocab.json',\n './Models\\\\pt\\\\merges.txt',\n './Models\\\\pt\\\\special_tokens_map.json',\n './Models\\\\pt\\\\added_tokens.json')"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "pytorch_model.save_pretrained(os.path.join(MODELS_PATH, \"pt\"))\n",
    "tokenizer.save_pretrained(os.path.join(MODELS_PATH, \"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "callable = tf.function(github_model.call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_function = callable.get_concrete_function([tf.TensorSpec([None, MAX_LEN], tf.int32, name=\"input_ids\"), tf.TensorSpec([None, MAX_LEN], tf.int32, name=\"attention_mask\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nWARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nINFO:tensorflow:Assets written to: ./Models\\Serving\\1\\assets\n"
    }
   ],
   "source": [
    "tf.saved_model.save(github_model, os.path.join(MODELS_PATH, \"Serving\", \"1\"), signatures=concrete_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try models on test issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = \"AppDomain.SetPrincipalPolicy(PrincipalPolicy.WindowsPrincipal) works only once. Setting the PrincipalPolicy on the current AppDomain to WindowsPrincipal works only for the first thread being started. Any subsequent thread has Thread.CurrentPrincipal evaluated to NULL.\"\n",
    "label = \"area-System.Security\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tf = tokenizer(issue, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='tf', add_special_tokens=True, return_token_type_ids=False)\n",
    "encoded_pt = tokenizer(issue, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt', add_special_tokens=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "All model checkpoint weights were used when initializing TFRobertaForSequenceClassification.\n\nAll the weights of TFRobertaForSequenceClassification were initialized from the model checkpoint at ./Models\\tf.\nIf your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
    }
   ],
   "source": [
    "github_model = TFRobertaForSequenceClassification.from_pretrained(os.path.join(MODELS_PATH, \"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(<tf.Tensor: shape=(1, 22), dtype=float32, numpy=\narray([[-1.3786112 , -2.113226  , -0.5003843 ,  3.178758  ,  0.22252794,\n        -1.0493124 ,  0.28553805,  0.8929118 , -0.50459427,  2.603506  ,\n        -0.94234836, -1.491632  ,  1.275148  ,  0.3172301 , -1.9975668 ,\n         0.8004527 ,  1.204209  ,  0.18403748,  1.8349376 ,  0.14521039,\n        -0.09998886, -1.1488237 ]], dtype=float32)>,)\n"
    }
   ],
   "source": [
    "tf_result = github_model(encoded_tf)\n",
    "print(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(tensor([[-1.3786, -2.1132, -0.5004,  3.1788,  0.2225, -1.0493,  0.2855,  0.8929,\n         -0.5046,  2.6035, -0.9423, -1.4916,  1.2751,  0.3172, -1.9976,  0.8005,\n          1.2042,  0.1840,  1.8349,  0.1452, -0.1000, -1.1488]],\n       grad_fn=<AddmmBackward>),)\n"
    }
   ],
   "source": [
    "pt_result = pytorch_model(input_ids=encoded_pt[\"input_ids\"], attention_mask=encoded_pt[\"attention_mask\"])\n",
    "print(pt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "np.allclose(tf_result[0], pt_result[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'area-System.Security'"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "lookup[str(np.argmax(tf_result))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to ONNX and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "pytorch_model.eval()\n",
    "torch.onnx.export(pytorch_model,               # model being run\n",
    "                  (encoded_pt[\"input_ids\"], encoded_pt[\"attention_mask\"]),  # model input (or a tuple for multiple inputs)\n",
    "                  os.path.join(MODELS_PATH, \"roberta_github_issues.onnx\"),   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=12,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input_ids', 'attention_mask'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input_ids' : {0 : 'batch_size'},\n",
    "                                'attention_mask' : {0 : 'batch_size'},\n",
    "                                'output' : {0 : 'batch_size'}}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(os.path.join(MODELS_PATH, \"roberta_github_issues.onnx\"))\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(os.path.join(MODELS_PATH, \"roberta_github_issues.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'attention_mask'"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "ort_session.get_inputs()[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(encoded_pt[\"input_ids\"]), ort_session.get_inputs()[1].name: to_numpy(encoded_pt[\"attention_mask\"])}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[array([[-1.3786125 , -2.1132238 , -0.5003866 ,  3.1787558 ,  0.22253025,\n        -1.0493083 ,  0.28553545,  0.8929088 , -0.5045951 ,  2.603507  ,\n        -0.942349  , -1.4916303 ,  1.2751487 ,  0.3172335 , -1.9975661 ,\n         0.800449  ,  1.2042091 ,  0.18403693,  1.8349365 ,  0.14520907,\n        -0.09998947, -1.1488235 ]], dtype=float32)]\n(tensor([[-1.3786, -2.1132, -0.5004,  3.1788,  0.2225, -1.0493,  0.2855,  0.8929,\n         -0.5046,  2.6035, -0.9423, -1.4916,  1.2751,  0.3172, -1.9976,  0.8005,\n          1.2042,  0.1840,  1.8349,  0.1452, -0.1000, -1.1488]],\n       grad_fn=<AddmmBackward>),)\n(<tf.Tensor: shape=(1, 22), dtype=float32, numpy=\narray([[-1.3786112 , -2.113226  , -0.5003843 ,  3.178758  ,  0.22252794,\n        -1.0493124 ,  0.28553805,  0.8929118 , -0.50459427,  2.603506  ,\n        -0.94234836, -1.491632  ,  1.275148  ,  0.3172301 , -1.9975668 ,\n         0.8004527 ,  1.204209  ,  0.18403748,  1.8349376 ,  0.14521039,\n        -0.09998886, -1.1488237 ]], dtype=float32)>,)\n"
    }
   ],
   "source": [
    "print(ort_outs)\n",
    "print(pt_result)\n",
    "print(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "index : 3, category : area-System.Security\n"
    }
   ],
   "source": [
    "index = np.argmax(to_numpy(pt_result[0]))\n",
    "print(f\"index : {index}, category : {lookup[str(index)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.functional.softmax(pt_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "area-System.Net             : 0.38% confidence\narea-Infrastructure         : 0.18% confidence\narea-System.ComponentModel  : 0.92% confidence\narea-System.Security        : 36.62% confidence\narea-System.Runtime         : 1.90% confidence\narea-System.IO              : 0.53% confidence\narea-System.Xml             : 2.03% confidence\narea-System.Collections     : 3.72% confidence\narea-System.Threading       : 0.92% confidence\narea-System.Reflection      : 20.60% confidence\narea-System.Memory          : 0.59% confidence\narea-System.Diagnostics     : 0.34% confidence\narea-Serialization          : 5.46% confidence\narea-System.Drawing         : 2.09% confidence\narea-Meta                   : 0.21% confidence\narea-System.Data            : 3.39% confidence\narea-Microsoft.CSharp       : 5.08% confidence\narea-System.Numerics        : 1.83% confidence\narea-System.Text            : 9.55% confidence\narea-System.Globalization   : 1.76% confidence\narea-System.Linq            : 1.38% confidence\narea-System.Console         : 0.48% confidence\n"
    }
   ],
   "source": [
    "for i, value in enumerate(softmax[0]):\n",
    "    print(f\"{lookup[str(i)] : <27} : {value.item() * 100 :.2f}% confidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}